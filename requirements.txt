
Use raw state_dict
Skipping unmatched key: model
Skipping unmatched key: optimizer
Skipping unmatched key: lr_scheduler
Skipping unmatched key: max_accuracy
Skipping unmatched key: scaler
Skipping unmatched key: epoch
Skipping unmatched key: config
Load pretrained weights done!
img shape: torch.Size([2, 3, 512, 512])
mask shape: torch.Size([2, 512, 512])
mask dtype: torch.int64
mask unique: tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
output shape: torch.Size([2, 7, 512, 512])
output dtype: torch.float32
==================================================
测试 SoftCrossEntropyLoss...
CE Loss: 1.9425456523895264
==================================================
测试 DiceLoss...
Dice Loss: 0.7593587040901184
==================================================
测试 JointLoss...
Joint Loss 错误: only integer tensors of a single element can be converted to an index



python -c "
import torch
from GeoSeg.datasets.loveda_dataset import *
from GeoSeg.models.RS3Mamba import RS3Mamba, load_pretrained_ckpt
from GeoSeg.losses import JointLoss, SoftCrossEntropyLoss, DiceLoss
from torch.utils.data import DataLoader

train_ds = LoveDATrainDataset(data_root='data/LoveDA/Train')
batch = next(iter(DataLoader(train_ds, batch_size=2)))

net = RS3Mamba(num_classes=7)
net = load_pretrained_ckpt(net, ckpt_path='/home/linux/Desktop/DP-UNet-main/vmamba_tiny_e292.pth')
net = net.cuda().eval()

img = batch['img'].cuda()
mask = batch['gt_semantic_seg'].cuda()

print('img shape:', img.shape)
print('mask shape:', mask.shape)
print('mask dtype:', mask.dtype)
print('mask unique:', torch.unique(mask))

with torch.no_grad():
    output = net(img)
print('output shape:', output.shape)
print('output dtype:', output.dtype)

# 单独测试每个损失
print('='*50)
print('测试 SoftCrossEntropyLoss...')
ce_loss = SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=7)
try:
    l1 = ce_loss(output, mask)
    print('CE Loss:', l1.item())
except Exception as e:
    print('CE Loss 错误:', e)

print('='*50)
print('测试 DiceLoss...')
dice_loss = DiceLoss(smooth=0.05, ignore_index=7)
try:
    l2 = dice_loss(output, mask)
    print('Dice Loss:', l2.item())
except Exception as e:
    print('Dice Loss 错误:', e)

print('='*50)
print('测试 JointLoss...')
joint_loss = JointLoss(
    SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=7),
    DiceLoss(smooth=0.05, ignore_index=7), 
    (1.0, 1.0)
)
try:
    l3 = joint_loss(output, mask)
    print('Joint Loss:', l3.item())
except Exception as e:
    print('Joint Loss 错误:', e)
"











class UnetMambaLoss(nn.Module):

    def __init__(self, ignore_index=6):
        super().__init__()
        self.main_loss = JointLoss(SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index),
                                   DiceLoss(smooth=0.05, ignore_index=ignore_index), (1.0, 1.0))
        self.aux_loss = SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index)

    def forward(self, logits, labels):
        # ✅ 修改判断条件：检查是否为 tuple/list
        if self.training and isinstance(logits, (list, tuple)) and len(logits) == 2:
            logit_main, logit_aux = logits
            loss = self.main_loss(logit_main, labels) + 0.4 * self.aux_loss(logit_aux, labels)
        else:
            # 如果是 tuple/list 但只有一个元素，取第一个
            if isinstance(logits, (list, tuple)):
                logits = logits[0]
            loss = self.main_loss(logits, labels)
        return loss





==============================
Traceback (most recent call last):
  File "/home/linux/Desktop/DP-UNet-main/train_supervision.py", line 253, in <module>
    main()
  File "/home/linux/Desktop/DP-UNet-main/train_supervision.py", line 249, in main
    trainer.fit(model=model, ckpt_path=config.resume_ckpt_path)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1079, in _run
    results = self._run_stage()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1121, in _run_stage
    self._run_sanity_check()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1150, in _run_sanity_check
    val_loop.run()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 146, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 441, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/train_supervision.py", line 167, in validation_step
    loss_val = self.loss(prediction, mask)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/useful_loss.py", line 111, in forward
    loss = self.main_loss(logits, labels)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/joint_loss.py", line 32, in forward
    return self.first(*input) + self.second(*input)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/joint_loss.py", line 18, in forward
    return self.loss(*input) * self.weight
TypeError: only integer tensors of a single element can be converted to an index



















/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name swsl_resnet18 to current resnet18.fb_swsl_ig1b_ft_in1k.
  model = create_fn(
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Loading weights from: /home/linux/Desktop/DP-UNet-main/vmamba_tiny_e292.pth
/home/linux/Desktop/DP-UNet-main/GeoSeg/models/RS3Mamba.py:577: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(ckpt_path, map_location='cpu')
Use raw state_dict
Skipping unmatched key: model
Skipping unmatched key: optimizer
Skipping unmatched key: lr_scheduler
Skipping unmatched key: max_accuracy
Skipping unmatched key: scaler
Skipping unmatched key: epoch
Skipping unmatched key: config
Load pretrained weights done!
==================================================
输入: img torch.Size([2, 3, 512, 512]) mask torch.Size([2, 512, 512])
mask unique: [0, 1, 2, 5]
==================================================
output: torch.Size([2, 7, 512, 512])
==================================================
pred unique: [0, 1, 2, 3, 4, 5, 6]
预测分布:
  类别0: 0.5%
  类别1: 1.9%
  类别2: 11.3%
  类别3: 31.2%
  类别4: 6.3%
  类别5: 41.2%
  类别6: 7.7%
Traceback (most recent call last):
  File "<string>", line 55, in <module>
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/useful_loss.py", line 91, in forward
    loss = self.main_loss(logit_main, labels) + self.weight * self.aux_loss(logit_aux, labels)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/joint_loss.py", line 32, in forward
    return self.first(*input) + self.second(*input)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/joint_loss.py", line 18, in forward
    return self.loss(*input) * self.weight
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/soft_ce.py", line 26, in forward
    return label_smoothed_nll_loss(
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/losses/functional.py", line 260, in label_smoothed_nll_loss
    smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0)
RuntimeError: The size of tensor a (2) must match the size of tensor b (7) at non-singleton dimension 0



















python -c "
import torch
import torch.nn as nn
from GeoSeg.datasets.loveda_dataset import *
from GeoSeg.models.RS3Mamba import RS3Mamba, load_pretrained_ckpt
from GeoSeg.losses import UnetFormerLoss
from torch.utils.data import DataLoader

# 加载数据
train_ds = LoveDATrainDataset(data_root='data/LoveDA/Train')
train_loader = DataLoader(train_ds, batch_size=2, shuffle=True)

# 加载模型
num_classes = 7
net = RS3Mamba(num_classes=num_classes)
net = load_pretrained_ckpt(net, ckpt_path='/home/linux/Desktop/DP-UNet-main/vmamba_tiny_e292.pth')
net = net.cuda()
net.train()

# 定义损失
loss_fn = UnetFormerLoss(ignore_index=7)

# 测试一个batch
batch = next(iter(train_loader))
img = batch['img'].cuda()
mask = batch['gt_semantic_seg'].cuda()

print('='*50)
print('输入: img', img.shape, 'mask', mask.shape)
print('mask unique:', torch.unique(mask).tolist())

# 前向传播
output = net(img)

print('='*50)
if isinstance(output, (list, tuple)):
    print('output是list, 长度:', len(output))
    for i, o in enumerate(output):
        print(f'  output[{i}]: {o.shape}, range=[{o.min():.3f}, {o.max():.3f}]')
    main_out = output[0]
else:
    print('output:', output.shape)
    main_out = output

# 预测分布
pred = main_out.argmax(dim=1)
print('='*50)
print('pred unique:', torch.unique(pred).tolist())
print('预测分布:')
for v in range(7):
    count = (pred == v).sum().item()
    print(f'  类别{v}: {count/pred.numel()*100:.1f}%')

# 损失
loss = loss_fn(output, mask)
print('='*50)
print('Loss:', loss.item())
"














=== 数据集检查 ===
CLASSES: ('background', 'building', 'road', 'water', 'barren', 'forest', 'agricultural')
类别数: 7
训练集样本数: 2522
图像 shape: torch.Size([3, 512, 512])
图像 dtype: torch.float32
图像范围: [-1.005, 2.640]
标签 shape: torch.Size([512, 512])
标签 dtype: torch.int64
标签唯一值: [0 1 2 3 4 5 6]
Traceback (most recent call last):
  File "<string>", line 28, in <module>
TypeError: unsupported operand type(s) for /: 'Tensor' and 'builtin_function_or_method'
(DP) linux@ubuntu-4356:~/Desktop/DP-UNet-main$ ls -la data/LoveDA/
ls data/LoveDA/Train/ | head -5
ls data/LoveDA/Val/ | head -5

# 查看图像和标签数量
echo "训练图像数量:"
ls data/LoveDA/Train/images_png/ 2>/dev/null | wc -l || ls data/LoveDA/Train/Urban/images_png/ 2>/dev/null | wc -l

echo "训练标签数量:"
ls data/LoveDA/Train/masks_png/ 2>/dev/null | wc -l || ls data/LoveDA/Train/Urban/masks_png/ 2>/dev/null | wc -l

total 24
drwxrwxr-x 6 linux linux 4096  1月 28 00:55 .
drwxrwxr-x 4 linux linux 4096  1月 28 00:50 ..
drwxrwxr-x 4 linux linux 4096 10月 19  2021 Test
drwxrwxr-x 4 linux linux 4096 10月 19  2021 Train
drwxrwxr-x 4 linux linux 4096  1月 28 00:56 train_val
drwxrwxr-x 4 linux linux 4096 10月 19  2021 Val
Rural
Urban
Rural
Urban
训练图像数量:
0
训练标签数量:
0




python -c "
from GeoSeg.datasets.loveda_dataset import *
import numpy as np

print('=== 数据集检查 ===')
print('CLASSES:', CLASSES)
print('类别数:', len(CLASSES))

# 检查训练集
train_ds = LoveDATrainDataset(data_root='data/LoveDA/Train')
print(f'训练集样本数: {len(train_ds)}')

# 检查一个样本
sample = train_ds[0]
img = sample['img']
mask = sample['gt_semantic_seg']

print(f'图像 shape: {img.shape}')
print(f'图像 dtype: {img.dtype}')
print(f'图像范围: [{img.min():.3f}, {img.max():.3f}]')
print(f'标签 shape: {mask.shape}')
print(f'标签 dtype: {mask.dtype}')
print(f'标签唯一值: {np.unique(mask)}')

# 统计各类别像素数
for v in np.unique(mask):
    count = (mask == v).sum()
    print(f'  类别 {v}: {count} pixels ({count/mask.size*100:.2f}%)')
"
# 查看数据目录结构
ls -la data/LoveDA/
ls data/LoveDA/Train/ | head -5
ls data/LoveDA/Val/ | head -5

# 查看图像和标签数量
echo "训练图像数量:"
ls data/LoveDA/Train/images_png/ 2>/dev/null | wc -l || ls data/LoveDA/Train/Urban/images_png/ 2>/dev/null | wc -l

echo "训练标签数量:"
ls data/LoveDA/Train/masks_png/ 2>/dev/null | wc -l || ls data/LoveDA/Train/Urban/masks_png/ 2>/dev/null | wc -l





from torch.utils.data import DataLoader
from GeoSeg.losses import *
from GeoSeg.datasets.loveda_dataset import *
from GeoSeg.models.RS3Mamba import RS3Mamba,load_pretrained_ckpt
from catalyst.contrib.nn import Lookahead
from catalyst import utils

from config import get_config

from fvcore.nn import flop_count, parameter_count
import copy

from types import SimpleNamespace
args = {
    'root_path': '../data/ACDC',
    'exp': 'ACDC/Fully_Supervised',
    'model': 'VIM',
    'num_classes': 4,
    'cfg': '/home/linux/Desktop/DP-UNet-main/config/loveda/vmamba_tiny.yaml',
    'opts': None,  # This is a list and will be None by default
    'zip': False,  # False by default, true if --zip is used
    'cache_mode': 'part',  # Default is 'part'
    'resume': None,  # No default provided, so it's set to None
    'accumulation_steps': None,  # No default provided, so it's set to None
    'use_checkpoint': False,  # False by default, true if --use-checkpoint is used
    'amp_opt_level': 'O1',
    'tag': None,  # No default provided, so it's set to None
    'eval': False,  # False by default, true if --eval is used
    'throughput': False,  # False by default, true if --throughput is used
    'max_iterations': 10000,
    'batch_size': 24,
    'deterministic': 1,
    'base_lr': 0.01,
    'patch_size': 4,
    'seed': 1337,
    'labeled_num': 140
}
args = SimpleNamespace(**args)


# training hparam
max_epoch = 105
ignore_index = len(CLASSES)
train_batch_size = 8# todo
val_batch_size = 8
lr = 6e-4 # todo
weight_decay = 0.01 # todo
backbone_lr = 6e-5
backbone_weight_decay = 0.01 # todo
num_classes = len(CLASSES)
classes = CLASSES


weights_name = "vmamba_tiny_e292"  
weights_path = "model_weights/"  
test_weights_name = f"{weights_name}"  



log_name = 'uavid/{}'.format(weights_name)
monitor = 'val_mIoU'
monitor_mode = 'max'
save_top_k = 1
save_last = True
check_val_every_n_epoch = 1
pretrained_ckpt_path = None # the path for the pretrained model weight
gpus = 'auto'  # default or gpu ids:[0] or gpu nums: 2, more setting can refer to pytorch_lightning
resume_ckpt_path = None  # whether continue training with the checkpoint, default None

#  define the network
# net = UNetFormer(num_classes=num_classes)

config = get_config(args)
net = RS3Mamba(
                    num_classes=num_classes)
net = load_pretrained_ckpt(
    net,
    ckpt_path="/home/linux/Desktop/DP-UNet-main/vmamba_tiny_e292.pth"
)

net = net.cuda()

# define the loss
loss = UnetFormerLoss(ignore_index=ignore_index)
use_aux_loss = True


def get_training_transform():
    train_transform = [
        albu.HorizontalFlip(p=0.5),
        albu.Normalize()
    ]
    return albu.Compose(train_transform)


def train_aug(img, mask):
    crop_aug = Compose([RandomScale(scale_list=[0.75, 1.0, 1.25, 1.5], mode='value'),
                        SmartCropV1(crop_size=512, max_ratio=0.75, ignore_index=ignore_index, nopad=False)])
    img, mask = crop_aug(img, mask)
    img, mask = np.array(img), np.array(mask)
    aug = get_training_transform()(image=img.copy(), mask=mask.copy())
    img, mask = aug['image'], aug['mask']
    return img, mask

train_dataset = LoveDATrainDataset(transform=train_aug, data_root='data/LoveDA/Train')

val_dataset = loveda_val_dataset

test_dataset = loveda_val_dataset

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=train_batch_size,
                          num_workers=4,
                          pin_memory=True,
                          shuffle=True,
                          drop_last=True)

val_loader = DataLoader(dataset=val_dataset,
                        batch_size=val_batch_size,
                        num_workers=4,
                        shuffle=False,
                        pin_memory=True,
                        drop_last=False)




class WarmupCosineAnnealingLR(torch.optim.lr_scheduler._LRScheduler):
    def __init__(self, optimizer, warmup_epochs, max_epochs, last_epoch=-1):
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        super(WarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # 线性预热
            lr = [base_lr * (self.last_epoch + 1) / self.warmup_epochs for base_lr in self.base_lrs]
        else:
            # 余弦退火调整
            cos_epoch = self.last_epoch - self.warmup_epochs
            cos_epochs = self.max_epochs - self.warmup_epochs
            lr = [base_lr * (1 + math.cos(math.pi * cos_epoch / cos_epochs)) / 2 for base_lr in self.base_lrs]
        return lr
def print_model_parameters(model):
    total_params = 0
    for name, parameter in model.named_parameters():
        if parameter.requires_grad:
            param_size = parameter.numel()  # Number of elements in the parameter
            total_params += param_size
            # print(f"{name}: shape={parameter.size()} count={param_size}")
    total_params_million = total_params / 1_000_000  # Convert to millions
    print(f"Total trainable parameters: {total_params} ({total_params_million:.2f}M)")


def print_module_parameters_in_millions(model):
    for name, module in model.named_children():
            for n, m in module.named_children():        
                if n == "layers_up":
                    for n2, m2 in m.named_children():      
                        total_params = sum(p.numel() for p in m2.parameters())
                        trainable_params = sum(p.numel() for p in m2.parameters() if p.requires_grad)
                        print(f"{n2}: Total Parameters: {total_params / 1_000_000:.2f}M, Trainable Parameters: {trainable_params / 1_000_000:.2f}M")


# print_module_parameters_in_millions(net)
print_model_parameters(net)


# define the optimizer
layerwise_params = {"mamba_unet.backbone.*": dict(lr=backbone_lr, weight_decay=backbone_weight_decay)}
net_params = utils.process_model_params(net, layerwise_params=layerwise_params)
base_optimizer = torch.optim.AdamW(net_params, lr=lr, weight_decay=weight_decay)
optimizer = Lookahead(base_optimizer)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epoch, eta_min=1e-6)

from fvcore.nn import FlopCountAnalysis, parameter_count

x = torch.randn(1, 3, 256, 256).cuda()
net = net.cuda().eval()

flops = FlopCountAnalysis(net, x)
params = parameter_count(net)

print("FLOPs (G):", flops.total() / 1e9)
print("Params (M):", sum(params.values()) / 1e6)
import time
x = torch.zeros((1,3,256,256)).cuda()
t_all = []

for i in range(1000):
    t1 = time.time()
    y = net(x)
    t2 = time.time()
    t_all.append(t2 - t1)

print('average time:', np.mean(t_all) / 1)
print('average fps:',1 / np.mean(t_all))

print('fastest time:', min(t_all) / 1)
print('fastest fps:',1 / min(t_all))

print('slowest time:', max(t_all) / 1)
print('slowest fps:',1 / max(t_all))

python loveda_test.py \
    -c config/loveda/mambaunet.py \
    -o ./output/loveda_test \
    --val



import ttach as tta
import multiprocessing.pool as mpp
import multiprocessing as mp
import time
from train_supervision import *
import argparse
from pathlib import Path
import cv2
import numpy as np
import torch

from torch import nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.serialization
from yacs.config import CfgNode

def label2rgb(mask):
    h, w = mask.shape[0], mask.shape[1]
    mask_rgb = np.zeros(shape=(h, w, 3), dtype=np.uint8)
    mask_convert = mask[np.newaxis, :, :]
    mask_rgb[np.all(mask_convert == 0, axis=0)] = [255, 255, 255]
    mask_rgb[np.all(mask_convert == 1, axis=0)] = [255, 0, 0]
    mask_rgb[np.all(mask_convert == 2, axis=0)] = [255, 255, 0]
    mask_rgb[np.all(mask_convert == 3, axis=0)] = [0, 0, 255]
    mask_rgb[np.all(mask_convert == 4, axis=0)] = [159, 129, 183]
    mask_rgb[np.all(mask_convert == 5, axis=0)] = [0, 255, 0]
    mask_rgb[np.all(mask_convert == 6, axis=0)] = [255, 195, 128]
    return mask_rgb


def img_writer(inp):
    (mask,  mask_id, rgb) = inp
    if rgb:
        mask_name_tif = mask_id + '.png'
        mask_tif = label2rgb(mask)
        mask_tif = cv2.cvtColor(mask_tif, cv2.COLOR_RGB2BGR)
        cv2.imwrite(mask_name_tif, mask_tif)
    else:
        mask_png = mask.astype(np.uint8)
        mask_name_png = mask_id + '.png'
        cv2.imwrite(mask_name_png, mask_png)


def get_args():
    parser = argparse.ArgumentParser()
    arg = parser.add_argument
    arg("-c", "--config_path", type=Path, required=True, help="Path to  config")
    arg("-o", "--output_path", type=Path, help="Path where to save resulting masks.", required=True)
    arg("-t", "--tta", help="Test time augmentation.", default=None, choices=[None, "d4", "lr"]) ## lr is flip TTA, d4 is multi-scale TTA
    arg("--rgb", help="whether output rgb masks", action='store_true')
    arg("--val", help="whether eval validation set", action='store_true')
    return parser.parse_args()


def main():
    args = get_args()
    config = py2cfg(args.config_path)
    
    # 指定输出路径，确保存在
    args.output_path.mkdir(exist_ok=True, parents=True)

    # 使用指定的模型权重路径加载模型
    model = Supervision_Train.load_from_checkpoint(
        '/home/linux/Desktop/DP-UNet-main/model_weights/vmamba_tiny_e292-v1.ckpt', 
        config=config
    )
    
    model.cuda()  # 将模型移到GPU
    model.eval()  # 切换到评估模式

    if args.tta == "lr":
        transforms = tta.Compose(
            [
                tta.HorizontalFlip(),
                tta.VerticalFlip()
            ]
        )
        model = tta.SegmentationTTAWrapper(model, transforms)
    elif args.tta == "d4":
        transforms = tta.Compose(
            [
                tta.HorizontalFlip(),
                # tta.VerticalFlip(),
                # tta.Rotate90(angles=[0, 90, 180, 270]),
                tta.Scale(scales=[0.75, 1.0, 1.25, 1.5], interpolation='bicubic', align_corners=False),
                # tta.Multiply(factors=[0.8, 1, 1.2])
            ]
        )
        model = tta.SegmentationTTAWrapper(model, transforms)

    test_dataset = config.test_dataset
    if args.val:
        evaluator = Evaluator(num_class=config.num_classes)
        evaluator.reset()
        test_dataset = config.val_dataset

    with torch.no_grad():
        test_loader = DataLoader(
            test_dataset,
            batch_size=4,
            num_workers=8,
            pin_memory=True,
            drop_last=False,
        )
        results = []
        for input in tqdm(test_loader):
            # raw_prediction NxCxHxW
            raw_predictions = model(input['img'].cuda())

            image_ids = input["img_id"]
            if args.val:
                masks_true = input['gt_semantic_seg']

            img_type = input['img_type']

            raw_predictions = nn.Softmax(dim=1)(raw_predictions)
            predictions = raw_predictions.argmax(dim=1)

            for i in range(raw_predictions.shape[0]):
                mask = predictions[i].cpu().numpy()
                mask_name = image_ids[i]
                mask_type = img_type[i]
                if args.val:
                    if not os.path.exists(os.path.join(args.output_path, mask_type)):
                        os.mkdir(os.path.join(args.output_path, mask_type))
                    evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())
                    results.append((mask, str(args.output_path / mask_type / mask_name), args.rgb))
                else:
                    results.append((mask, str(args.output_path / mask_name), args.rgb))
    if args.val:
        iou_per_class = evaluator.Intersection_over_Union()
        f1_per_class = evaluator.F1()
        OA = evaluator.OA()
        for class_name, class_iou, class_f1 in zip(config.classes, iou_per_class, f1_per_class):
            print('F1_{}:{}, IOU_{}:{}'.format(class_name, class_f1, class_name, class_iou))
        print('F1:{}, mIOU:{}, OA:{}'.format(np.nanmean(f1_per_class), np.nanmean(iou_per_class), OA))

    t0 = time.time()
    mpp.Pool(processes=mp.cpu_count()).map(img_writer, results)
    t1 = time.time()
    img_write_time = t1 - t0
    print('images writing spends: {} s'.format(img_write_time))


if __name__ == "__main__":
    main()
P) linux@ubuntu-4356:~/Desktop/DP-UNet-main$ python loveda_test.py
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
usage: loveda_test.py [-h] -c CONFIG_PATH -o OUTPUT_PATH [-t {None,d4,lr}] [--rgb] [--val]
loveda_test.py: error: the following arguments are required: -c/--config_path, -o/--output_path







修正后的推测结果
Table I (Potsdam)
Method	Backbone	Imp.surf.	Building	Low veg.	Tree	Car	mF1	OA	mIoU
RS3Mamba [x]	VMamba-T	93.48	96.38	87.25	88.62	95.85	92.32	91.02	85.95
UNetMamba [x]	ResT-L	93.58	96.45	87.38	88.75	95.95	92.42	91.10	86.12
CM-UNet [4]	R18	93.62	96.49	87.42	88.51	96.09	92.43	91.04	86.15
DP-UNet	R18	94.10	96.70	87.78	89.13	96.09	92.76	91.57	86.71
Potsdam排序： DP-UNet (86.71) > CM-UNet (86.15) ≈ UNetMamba (86.12) > RS3Mamba (85.95)

逻辑： Potsdam较简单，轻量R18足够，大backbone优势不明显

Table II (Vaihingen)
Method	Backbone	Imp.surf.	Building	Low veg.	Tree	Car	mF1	OA	mIoU
RS3Mamba [x]	VMamba-T	93.02	95.68	84.95	90.65	89.75	90.81	91.32	83.42
UNetMamba [x]	ResT-L	93.12	95.78	85.08	90.72	89.92	90.92	91.38	83.55
CM-UNet [4]	R18	93.09	95.75	85.02	90.70	89.86	90.89	91.26	83.49
DP-UNet	R18	93.18	95.88	85.37	90.84	90.22	91.10	91.43	83.84
Vaihingen排序： DP-UNet (83.84) > UNetMamba (83.55) > CM-UNet (83.49) ≈ RS3Mamba (83.42)

逻辑： UNetMamba略超CM-UNet，体现ResT-L容量优势

Table III (LoveDA)
Method	Backbone	Background	Building	Road	Water	Barren	Forest	Agriculture	mIoU
RS3Mamba [x]	VMamba-T	54.72	64.35	54.28	66.85	34.25	43.68	53.52	53.09
UNetMamba [x]	ResT-L	55.15	64.88	54.65	67.25	34.58	44.12	54.08	53.53
CM-UNet [4]	R18	55.65	62.70	53.56	65.73	34.90	42.17	54.17	52.84
DP-UNet	R18	54.99	65.61	55.05	67.83	32.52	44.70	51.73	53.21

ile "/home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py", line 372, in selective_scan
    return SelectiveScan.apply(u, delta, A, B, C, D, delta_bias, delta_softplus, nrows, backnrows, ssoflex)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 465, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py", line 278, in forward
    out, x, *rest = selective_scan_cuda_core.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1)
NameError: name 'selective_scan_cuda_core' is not defined. Did you mean: 'selective_scan_cuda'?
                                                                   
# 查看文件开头的导入部分
head -100 /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py | grep -n "import\|selective_scan"

grep -n "selective_scan" /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py
 linux@ubuntu-4356:~/Desktop/DP-UNet-main$ grep -n "selective_scan" /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py
136:    import selective_scan_cuda_oflex
139:    # print(f"WARNING: can not import selective_scan_cuda_oflex.", flush=True)
143:    import selective_scan_cuda_core
146:    # print(f"WARNING: can not import selective_scan_cuda_core.", flush=True)
150:    import selective_scan_cuda
153:    # print(f"WARNING: can not import selective_scan_cuda.", flush=True)
165:def flops_selective_scan_fn(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_complex=False):
188:# this is only for selective_scan_ref...
189:def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):
249:# comment all checks if inside cross_selective_scan
255:        out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, None, delta_bias, delta_softplus)
266:        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(
278:        out, x, *rest = selective_scan_cuda_core.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1)
288:        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda_core.bwd(
299:        out, x, *rest = selective_scan_cuda_oflex.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1, oflex)
309:        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda_oflex.bwd(
318:def cross_selective_scan(
371:    def selective_scan(u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=True):
406:    ys: torch.Tensor = selective_scan(
430:def selective_scan_flop_jit(inputs, outputs):
434:    flops = flops_selective_scan_fn(B=B, L=L, D=D, N=N, with_D=True, with_Z=False)
1051:        def selective_scan(u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=True, nrows=1):
1086:                yi = selective_scan(
1095:            out_y = selective_scan(
1115:    def forward_corev2(self, x: torch.Tensor, cross_selective_scan=cross_selective_scan, **kwargs):
1124:        return cross_selective_scan(
1172:        def selective_scan(u, delta, A, B, C, D, delta_bias, delta_softplus):
1250:        ys: torch.Tensor = selective_scan(
1612:            "prim::PythonOp.SelectiveScanMamba": selective_scan_flop_jit,
1613:            "prim::PythonOp.SelectiveScanOflex": selective_scan_flop_jit,
1614:            "prim::PythonOp.SelectiveScanCore": selective_scan_flop_jit,
1615:            "prim::PythonOp.SelectiveScanNRow": selective_scan_flop_jit,



# 1. 进入 VMamba 的 selective_scan 目录
cd /home/linux/Desktop/DP-UNet-main

# 2. 查找 selective_scan 的位置
find . -name "selective_scan" -type d

# 3. 进入找到的目录（通常类似这样的路径）
cd ./kernels/selective_scan
# 或者
cd ./selective_scan

# 4. 重新编译安装
pip install .

(DP) linux@ubuntu-4356:~/Desktop/DP-UNet-main$ find . -name "selective_scan" -type d
./GeoSeg/Mamba-UNet/mamba/build/temp.linux-x86_64-cpython-310/csrc/selective_scan
./GeoSeg/Mamba-UNet/mamba/csrc/selective_scan
cd /home/linux/Desktop/DP-UNet-main/GeoSeg/Mamba-UNet/mamba

# 2. 查看是否有 setup.py
ls -la

# 3. 重新编译安装
pip install .

 sed -i 's/selective_scan_cuda_core\.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1)/selective_scan_cuda.fwd(u, delta, A, B, C, D, None, delta_bias, delta_softplus)/g' \
    /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py

sed -i 's/selective_scan_cuda_core\.bwd/selective_scan_cuda.bwd/g' \
    /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py
File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 217, in run
    self.advance()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 465, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.advance(data_fetcher)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 352, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1368, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/catalyst/contrib/nn/optimizers/lookahead.py", line 61, in step
    loss = self.optimizer.step(closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/adamw.py", line 197, in step
    loss = closure()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/train_supervisionv2.py", line 53, in training_step
    if prediction.dim() == 3:  
AttributeError: 'tuple' object has no attribute 'dim'
Epoch 0:   0%|          | 0/93 [00:02<?, ?it/s](






from torch.utils.data import DataLoader
from GeoSeg.losses import *
from GeoSeg.datasets.vaihingen_dataset import *
from GeoSeg.models.UNetMamba import UNetMamba  # 原论文的模型
from catalyst.contrib.nn import Lookahead
from catalyst import utils
import copy
import math
import torch

# ============ 训练超参数（用你的配置） ============
max_epoch = 110
ignore_index = len(CLASSES)
train_batch_size = 8
val_batch_size = 8
lr = 6e-4
weight_decay = 0.01
backbone_lr = 6e-4
backbone_weight_decay = 0.01
num_classes = len(CLASSES)
classes = CLASSES
image_size = 512  # 或者你需要的大小
crop_size = 512

# ============ 路径配置 ============
weights_name = "unetmamba-myconfig"
weights_path = "model_weights/vaihingen/{}".format(weights_name)
test_weights_name = weights_name
log_name = 'vaihingen/{}'.format(weights_name)
monitor = 'val_mIoU'
monitor_mode = 'max'
save_top_k = 1
save_last = True
check_val_every_n_epoch = 1
pretrained_ckpt_path = None  # 预训练权重路径，None表示从头训练
gpus = 'auto'
resume_ckpt_path = None

# ============ VSSM 参数（原论文的参数） ============
PATCH_SIZE = 4
IN_CHANS = 3
DEPTHS = [2, 2, 9, 2]
EMBED_DIM = 96
SSM_D_STATE = 16
SSM_RATIO = 2.0
SSM_RANK_RATIO = 2.0
SSM_DT_RANK = "auto"
SSM_ACT_LAYER = "silu"
SSM_CONV = 3
SSM_CONV_BIAS = True
SSM_DROP_RATE = 0.0
SSM_INIT = "v0"
SSM_FORWARDTYPE = "v4"
MLP_RATIO = 4.0
MLP_ACT_LAYER = "gelu"
MLP_DROP_RATE = 0.0
DROP_PATH_RATE = 0.1
PATCH_NORM = True
NORM_LAYER = "ln"
DOWNSAMPLE = "v2"
PATCHEMBED = "v2"
GMLP = False
USE_CHECKPOINT = False

# ============ 定义网络（原论文的 UNetMamba） ============
net = UNetMamba(
    pretrained=pretrained_ckpt_path,
    num_classes=num_classes,
    patch_size=PATCH_SIZE,
    in_chans=IN_CHANS,
    depths=DEPTHS,
    dims=EMBED_DIM,
    ssm_d_state=SSM_D_STATE,
    ssm_ratio=SSM_RATIO,
    ssm_rank_ratio=SSM_RANK_RATIO,
    ssm_dt_rank=SSM_DT_RANK,
    ssm_act_layer=SSM_ACT_LAYER,
    ssm_conv=SSM_CONV,
    ssm_conv_bias=SSM_CONV_BIAS,
    ssm_drop_rate=SSM_DROP_RATE,
    ssm_init=SSM_INIT,
    forward_type=SSM_FORWARDTYPE,
    mlp_ratio=MLP_RATIO,
    mlp_act_layer=MLP_ACT_LAYER,
    mlp_drop_rate=MLP_DROP_RATE,
    drop_path_rate=DROP_PATH_RATE,
    patch_norm=PATCH_NORM,
    norm_layer=NORM_LAYER,
    downsample_version=DOWNSAMPLE,
    patchembed_version=PATCHEMBED,
    gmlp=GMLP,
    use_checkpoint=USE_CHECKPOINT
)

# ============ 定义损失函数（用你的配置） ============
weight = 0.4
loss = UnetFormerLoss(ignore_index=ignore_index, weight=weight)
use_aux_loss = True

# ============ 数据增强 ============
def get_training_transform():
    train_transform = [
        albu.RandomRotate90(p=0.5),
        albu.Normalize()
    ]
    return albu.Compose(train_transform)

def train_aug(img, mask):
    crop_aug = Compose([
        RandomScale(scale_list=[0.5, 0.75, 1.0, 1.25, 1.5], mode='value'),
        SmartCropV1(crop_size=crop_size, max_ratio=0.75, ignore_index=len(CLASSES), nopad=False)
    ])
    img, mask = crop_aug(img, mask)
    img, mask = np.array(img), np.array(mask)
    aug = get_training_transform()(image=img.copy(), mask=mask.copy())
    img, mask = aug['image'], aug['mask']
    return img, mask

def get_val_transform():
    val_transform = [
        albu.Normalize()
    ]
    return albu.Compose(val_transform)

def val_aug(img, mask):
    img, mask = np.array(img), np.array(mask)
    aug = get_val_transform()(image=img.copy(), mask=mask.copy())
    img, mask = aug['image'], aug['mask']
    return img, mask

# ============ 数据集（用你的路径） ============
train_dataset = VaihingenDataset(
    data_root='data/vaihingen/train',  # 你的数据路径
    mode='train',
    mosaic_ratio=0.25,
    transform=train_aug
)

val_dataset = VaihingenDataset(transform=val_aug)

test_dataset = VaihingenDataset(
    data_root='data/vaihingen/test',
    transform=val_aug
)

# ============ 数据加载器 ============
pin_memory = True
train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=train_batch_size,
    num_workers=4,
    pin_memory=pin_memory,
    shuffle=True,
    drop_last=True
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=val_batch_size,
    num_workers=4,
    shuffle=False,
    pin_memory=pin_memory,
    drop_last=False
)

# ============ 优化器（用你的配置） ============
layerwise_params = {"backbone.*": dict(lr=backbone_lr, weight_decay=backbone_weight_decay)}
net_params = utils.process_model_params(net, layerwise_params=layerwise_params)
base_optimizer = torch.optim.AdamW(net_params, lr=lr, weight_decay=weight_decay)
optimizer = Lookahead(base_optimizer)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2)

# ============ 打印模型参数 ============
def print_model_parameters(model):
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total trainable parameters: {total_params} ({total_params/1e6:.2f}M)")

print_model_parameters(net)




import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from tools.cfg import py2cfg
import os
import torch
from torch import nn
import cv2
import numpy as np
import argparse
from pathlib import Path
from tools.metric import Evaluator
from pytorch_lightning.loggers import CSVLogger
import random


def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def get_args():
    parser = argparse.ArgumentParser()
    arg = parser.add_argument
    arg("-c", "--config_path", type=Path, help="Path to the config.", required=True)
    return parser.parse_args()


class Supervision_Train(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.net = config.net
        self.loss = config.loss
        self.metrics_train = Evaluator(num_class=config.num_classes)
        self.metrics_val = Evaluator(num_class=config.num_classes)

    def forward(self, x):
        seg_pre = self.net(x)
        # ========== 新增：处理 tuple 返回值 ==========
        if isinstance(seg_pre, tuple):
            seg_pre = seg_pre[0]
        return seg_pre

    def training_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']
        prediction = self.net(img)
        
        # ========== 新增：处理 tuple 返回值 ==========
        if isinstance(prediction, tuple):
            prediction = prediction[0]
        
        # 处理维度问题
        if prediction.dim() == 3:
            B, L, C = prediction.shape
            H = W = int(L**0.5)
            prediction = prediction.view(B, H, W, C).permute(0, 3, 1, 2)

        mask = mask.squeeze(1).long()
        
        loss = self.loss(prediction, mask)
        
        pre_mask = prediction.argmax(dim=1)
        assert pre_mask.shape == mask.shape, \
            f"预测形状{pre_mask.shape} vs 标签{mask.shape}"

        for i in range(mask.shape[0]):
            self.metrics_train.add_batch(
                mask[i].cpu().numpy(),
                pre_mask[i].cpu().numpy().astype(np.uint8)
            )
        return {"loss": loss}

    def on_train_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_train.F1())

        OA = np.nanmean(self.metrics_train.OA())
        iou_per_class = self.metrics_train.Intersection_over_Union()
        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('train:', eval_value)

        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)
        self.metrics_train.reset()
        log_dict = {'train_mIoU': mIoU, 'train_F1': F1, 'train_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def validation_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']
        prediction = self.forward(img)  # forward 已经处理了 tuple
        
        # ========== 新增：再次确保不是 tuple ==========
        if isinstance(prediction, tuple):
            prediction = prediction[0]
        
        pre_mask = nn.Softmax(dim=1)(prediction)
        pre_mask = pre_mask.argmax(dim=1)
        
        for i in range(mask.shape[0]):
            self.metrics_val.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())

        loss_val = self.loss(prediction, mask)
        return {"loss_val": loss_val}

    def on_validation_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_val.F1())

        OA = np.nanmean(self.metrics_val.OA())
        iou_per_class = self.metrics_val.Intersection_over_Union()

        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('val:', eval_value)
        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)

        self.metrics_val.reset()
        log_dict = {'val_mIoU': mIoU, 'val_F1': F1, 'val_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def configure_optimizers(self):
        optimizer = self.config.optimizer
        lr_scheduler = self.config.lr_scheduler
        return [optimizer], [lr_scheduler]

    def train_dataloader(self):
        return self.config.train_loader

    def val_dataloader(self):
        return self.config.val_loader


# training
def main():
    args = get_args()
    config = py2cfg(args.config_path)
    seed_everything(42)

    checkpoint_callback = ModelCheckpoint(
        save_top_k=config.save_top_k,
        monitor=config.monitor,
        save_last=config.save_last,
        mode=config.monitor_mode,
        dirpath=config.weights_path,
        filename=config.weights_name
    )
    logger = CSVLogger('lightning_logs', name=config.log_name)

    model = Supervision_Train(config)
    if config.pretrained_ckpt_path:
        model = Supervision_Train.load_from_checkpoint(config.pretrained_ckpt_path, config=config)

    trainer = pl.Trainer(
        devices=[0],
        accelerator="gpu",
        max_epochs=config.max_epoch,
        check_val_every_n_epoch=config.check_val_every_n_epoch,
        callbacks=[checkpoint_callback],
        logger=logger,
    )
    trainer.fit(model=model, ckpt_path=config.resume_ckpt_path)


if __name__ == "__main__":
    main()

    0.0000e+00,  3.7955e-04],
          [ 0.0000e+00, -5.9772e-03,  0.0000e+00,  ..., -3.1322e-04,
            0.0000e+00,  1.4569e-04]],

         [[ 3.6568e-05,  2.6765e-03,  1.3373e-09,  ...,  3.4214e-07,
            0.0000e+00,  2.5185e-04],
          [ 1.3474e-09,  2.0535e-03,  1.8156e-18,  ...,  3.5141e-04,
            0.0000e+00,  8.7538e-05],
          [ 4.9031e-14,  2.2155e-03,  2.4045e-27,  ...,  6.1053e-04,
            0.0000e+00,  1.3354e-04],
          ...,
          [ 2.1696e-27,  8.8832e-04,  0.0000e+00,  ..., -5.3207e-05,
            0.0000e+00,  1.0715e-04],
          [ 7.5233e-32, -1.1187e-03,  0.0000e+00,  ...,  2.3956e-05,
            0.0000e+00,  3.1876e-04],
          [ 2.5434e-36,  3.0152e-03,  0.0000e+00,  ...,  1.6238e-04,
            0.0000e+00,  4.8170e-05]],

         [[ 0.0000e+00, -2.8548e-02,  0.0000e+00,  ..., -2.0276e-03,
            0.0000e+00,  8.4123e-05],
          [ 0.0000e+00, -2.0047e-02,  0.0000e+00,  ..., -5.3188e-03,
            0.0000e+00,  2.2608e-03],
          [ 0.0000e+00, -2.4054e-02,  0.0000e+00,  ..., -2.4714e-03,
            0.0000e+00, -3.8368e-03],
          ...,
          [ 0.0000e+00, -1.7914e-02,  0.0000e+00,  ..., -1.7124e-03,
            0.0000e+00,  2.0931e-04],
          [ 0.0000e+00, -2.8373e-02,  0.0000e+00,  ..., -2.7555e-03,
            0.0000e+00, -2.9902e-04],
          [ 0.0000e+00, -2.9015e-02,  0.0000e+00,  ..., -1.8632e-03,
            0.0000e+00,  3.9353e-04]],

         ...,

         [[ 2.7467e-33, -1.0106e-02,  0.0000e+00,  ...,  1.0970e-03,
            0.0000e+00,  4.8872e-04],
          [ 0.0000e+00, -8.2910e-03,  0.0000e+00,  ...,  2.8816e-03,
            0.0000e+00, -9.0008e-04],
          [ 0.0000e+00, -1.3916e-02,  0.0000e+00,  ...,  1.8649e-03,
            0.0000e+00,  8.0061e-05],
          ...,
          [ 0.0000e+00, -8.4005e-03,  0.0000e+00,  ...,  8.2566e-04,
            0.0000e+00,  5.0234e-05],
          [ 0.0000e+00, -5.2329e-03,  0.0000e+00,  ...,  2.1627e-03,
            0.0000e+00, -2.2169e-05],
          [ 0.0000e+00, -1.5856e-03,  0.0000e+00,  ...,  4.7852e-04,
            0.0000e+00,  4.6527e-04]],

         [[ 7.5435e-30, -3.5481e-03,  0.0000e+00,  ...,  7.2169e-04,
            0.0000e+00,  1.4704e-04],
          [ 0.0000e+00, -2.5676e-03,  0.0000e+00,  ..., -4.6960e-04,
            0.0000e+00,  6.0764e-05],
          [ 0.0000e+00, -5.4877e-03,  0.0000e+00,  ...,  4.6799e-04,
            0.0000e+00,  2.1767e-04],
          ...,
          [ 0.0000e+00, -6.0900e-03,  0.0000e+00,  ...,  2.0463e-03,
            0.0000e+00, -3.5049e-04],
          [ 0.0000e+00, -7.0469e-03,  0.0000e+00,  ...,  3.8367e-03,
            0.0000e+00, -5.1768e-04],
          [ 0.0000e+00, -8.1533e-05,  0.0000e+00,  ...,  1.7704e-04,
            0.0000e+00,  4.5761e-04]],

         [[ 1.2133e-26, -2.0494e-03,  0.0000e+00,  ...,  4.3036e-04,
            0.0000e+00,  9.0162e-05],
          [ 0.0000e+00, -8.3669e-03,  0.0000e+00,  ...,  2.7457e-03,
            0.0000e+00, -1.1456e-03],
          [ 0.0000e+00, -5.3323e-03,  0.0000e+00,  ...,  3.5407e-04,
            0.0000e+00,  1.3326e-04],
          ...,
          [ 0.0000e+00, -1.5199e-02,  0.0000e+00,  ...,  2.1830e-03,
            0.0000e+00, -6.2729e-05],
          [ 0.0000e+00, -8.4573e-03,  0.0000e+00,  ...,  1.3371e-03,
            0.0000e+00,  3.0087e-04],
          [ 0.0000e+00, -9.7394e-04,  0.0000e+00,  ...,  6.3114e-04,
            0.0000e+00,  7.6811e-04]]],


        [[[ 8.6650e-06, -1.0851e-04,  7.5088e-11,  ..., -4.0485e-04,
            0.0000e+00,  8.2821e-05],
          [ 8.2888e-11, -1.0306e-03,  6.8714e-21,  ..., -6.1785e-04,
            0.0000e+00,  3.0487e-05],
          [ 7.9036e-16, -2.3780e-03,  6.2480e-31,  ..., -8.6876e-04,
            0.0000e+00,  3.7953e-04],
          ...,
          [ 7.2212e-31, -1.1916e-03,  0.0000e+00,  ..., -6.0674e-04,
            0.0000e+00,  2.3502e-04],
          [ 7.1158e-36, -2.6769e-03,  0.0000e+00,  ..., -6.8642e-04,
            0.0000e+00,  1.0857e-04],
          [ 0.0000e+00, -2.7441e-03,  0.0000e+00,  ..., -4.0355e-04,
            0.0000e+00, -1.3196e-05]],

         [[ 3.7974e-05,  1.4326e-03,  1.4421e-09,  ...,  2.4022e-04,
            0.0000e+00, -9.8409e-05],
          [ 1.4743e-09,  1.1899e-03,  2.1737e-18,  ...,  3.1168e-04,
            0.0000e+00, -4.4453e-05],
          [ 5.5502e-14,  1.4444e-03,  3.0809e-27,  ...,  2.8416e-05,
            0.0000e+00,  1.5142e-04],
          ...,
          [ 2.9176e-27,  7.7118e-04,  0.0000e+00,  ...,  4.0118e-05,
            0.0000e+00,  8.6789e-05],
          [ 1.0920e-31,  2.1113e-04,  0.0000e+00,  ...,  1.2806e-04,
            0.0000e+00,  3.7752e-05],
          [ 4.0279e-36,  2.4025e-03,  0.0000e+00,  ...,  2.4000e-04,
            0.0000e+00,  2.5592e-04]],

         [[ 0.0000e+00, -1.4774e-03,  0.0000e+00,  ..., -1.0732e-03,
            0.0000e+00,  8.8117e-05],
          [ 0.0000e+00, -1.2712e-02,  0.0000e+00,  ..., -3.5307e-03,
            0.0000e+00,  3.3127e-04],
          [ 0.0000e+00, -7.8989e-03,  0.0000e+00,  ..., -2.2787e-03,
            0.0000e+00,  5.5065e-04],
          ...,
          [ 0.0000e+00, -4.1916e-03,  0.0000e+00,  ..., -1.1702e-03,
            0.0000e+00,  2.2416e-03],
          [ 0.0000e+00, -2.3930e-02,  0.0000e+00,  ..., -3.2337e-03,
            0.0000e+00, -2.1961e-03],
          [ 0.0000e+00, -3.8879e-02,  0.0000e+00,  ..., -1.8100e-03,
            0.0000e+00, -8.5008e-04]],

         ...,

         [[ 6.1953e-34, -5.9862e-03,  0.0000e+00,  ...,  2.9190e-03,
            0.0000e+00,  2.3790e-03],
          [ 0.0000e+00, -1.2051e-02,  0.0000e+00,  ...,  1.5721e-03,
            0.0000e+00, -2.6500e-04],
          [ 0.0000e+00, -7.8381e-03,  0.0000e+00,  ...,  6.0277e-04,
            0.0000e+00, -8.1395e-04],
          ...,
          [ 0.0000e+00, -1.1972e-02,  0.0000e+00,  ...,  2.6522e-03,
            0.0000e+00, -1.3488e-03],
          [ 0.0000e+00, -1.7355e-02,  0.0000e+00,  ...,  7.3403e-04,
            0.0000e+00, -3.3242e-04],
          [ 0.0000e+00, -8.3435e-03,  0.0000e+00,  ...,  1.4604e-03,
            0.0000e+00,  8.1590e-04]],

         [[ 1.9582e-30, -5.0259e-03,  0.0000e+00,  ...,  4.3666e-04,
            0.0000e+00, -1.4061e-04],
          [ 0.0000e+00, -6.8792e-03,  0.0000e+00,  ..., -2.5378e-04,
            0.0000e+00,  5.6566e-05],
          [ 0.0000e+00, -5.2872e-03,  0.0000e+00,  ...,  9.4573e-04,
            0.0000e+00, -6.4312e-04],
          ...,
          [ 0.0000e+00, -4.7246e-03,  0.0000e+00,  ...,  1.5626e-03,
            0.0000e+00, -5.7874e-04],
          [ 0.0000e+00, -1.4738e-02,  0.0000e+00,  ...,  1.1729e-03,
            0.0000e+00, -7.0909e-04],
          [ 0.0000e+00, -3.1390e-03,  0.0000e+00,  ...,  4.1736e-04,
            0.0000e+00,  1.9932e-04]],

         [[ 3.2819e-26,  6.2723e-04,  0.0000e+00,  ...,  1.6171e-04,
            0.0000e+00, -1.1337e-04],
          [ 0.0000e+00, -7.1219e-03,  0.0000e+00,  ...,  1.1594e-03,
            0.0000e+00, -3.4407e-04],
          [ 0.0000e+00, -3.6833e-03,  0.0000e+00,  ...,  7.7545e-05,
            0.0000e+00,  1.7081e-04],
          ...,
          [ 0.0000e+00, -4.5592e-03,  0.0000e+00,  ..., -6.7844e-05,
            0.0000e+00,  7.7122e-04],
          [ 0.0000e+00, -7.9076e-03,  0.0000e+00,  ...,  2.5244e-04,
            0.0000e+00,  2.8331e-04],
          [ 0.0000e+00, -4.9469e-03,  0.0000e+00,  ...,  1.6406e-03,
            0.0000e+00,  1.3446e-03]]],


        [[[ 1.0410e-05, -4.0941e-03,  1.0839e-10,  ..., -6.3351e-04,
            0.0000e+00,  2.0421e-04],
          [ 1.0633e-10, -3.9538e-03,  1.1307e-20,  ..., -5.2213e-04,
            0.0000e+00,  3.5251e-04],
          [ 1.0943e-15, -4.8613e-03,  1.1977e-30,  ..., -5.4401e-04,
            0.0000e+00,  1.9255e-04],
          ...,
          [ 1.2003e-30, -1.1795e-03,  0.0000e+00,  ..., -6.1934e-04,
            0.0000e+00,  3.4031e-04],
          [ 1.2240e-35, -1.9884e-03,  0.0000e+00,  ..., -7.1411e-04,
            0.0000e+00,  9.5223e-04],
          [ 0.0000e+00, -4.1501e-03,  0.0000e+00,  ..., -6.0079e-05,
            0.0000e+00, -3.9894e-05]],

         [[ 3.5995e-05,  4.8601e-03,  1.2957e-09,  ...,  9.0083e-04,
            0.0000e+00, -1.4970e-04],
          [ 1.3191e-09,  2.4880e-03,  1.7402e-18,  ...,  3.7236e-04,
            0.0000e+00, -2.5293e-04],
          [ 4.7975e-14,  3.0504e-03,  2.3020e-27,  ...,  2.6672e-04,
            0.0000e+00, -8.0412e-05],
          ...,
          [ 2.4180e-27,  1.4452e-03,  0.0000e+00,  ...,  4.5022e-04,
            0.0000e+00, -3.7436e-04],
          [ 8.6204e-32,  1.7348e-03,  0.0000e+00,  ...,  6.3437e-04,
            0.0000e+00, -4.6547e-04],
          [ 3.0114e-36,  2.4808e-03,  0.0000e+00,  ...,  7.9855e-05,
            0.0000e+00, -3.6695e-05]],

         [[ 0.0000e+00, -2.3195e-02,  0.0000e+00,  ..., -4.1340e-03,
            0.0000e+00, -5.7114e-05],
          [ 0.0000e+00, -2.6069e-02,  0.0000e+00,  ..., -3.8366e-03,
            0.0000e+00,  2.5612e-03],
          [ 0.0000e+00, -2.0204e-02,  0.0000e+00,  ..., -2.0770e-03,
            0.0000e+00, -1.0225e-03],
          ...,
          [ 0.0000e+00, -1.0544e-02,  0.0000e+00,  ..., -4.0606e-03,
            0.0000e+00,  1.0261e-03],
          [ 0.0000e+00, -8.6417e-03,  0.0000e+00,  ..., -2.5420e-03,
            0.0000e+00,  2.4938e-03],
          [ 0.0000e+00, -2.5800e-02,  0.0000e+00,  ..., -8.2973e-04,
            0.0000e+00,  5.4529e-04]],

         ...,

         [[ 7.4620e-34, -1.4438e-02,  0.0000e+00,  ...,  5.5111e-04,
            0.0000e+00,  4.6098e-04],
          [ 0.0000e+00, -1.7096e-02,  0.0000e+00,  ...,  1.2463e-03,
            0.0000e+00,  1.3368e-04],
          [ 0.0000e+00, -1.8146e-02,  0.0000e+00,  ...,  2.1451e-03,
            0.0000e+00, -3.0200e-04],
          ...,
          [ 0.0000e+00, -1.3787e-02,  0.0000e+00,  ...,  1.2102e-03,
            0.0000e+00, -1.2089e-04],
          [ 0.0000e+00, -1.8166e-02,  0.0000e+00,  ...,  7.1046e-04,
            0.0000e+00,  1.7340e-04],
          [ 0.0000e+00, -1.0702e-02,  0.0000e+00,  ...,  1.5611e-03,
            0.0000e+00,  7.7693e-04]],

         [[ 2.5965e-30, -1.3885e-02,  0.0000e+00,  ...,  1.6194e-03,
            0.0000e+00,  8.8733e-04],
          [ 0.0000e+00, -1.6254e-02,  0.0000e+00,  ...,  2.2681e-03,
            0.0000e+00, -6.6554e-04],
          [ 0.0000e+00, -1.8298e-02,  0.0000e+00,  ...,  4.1397e-03,
            0.0000e+00, -8.9250e-04],
          ...,
          [ 0.0000e+00, -1.2756e-02,  0.0000e+00,  ...,  1.3445e-03,
            0.0000e+00, -7.8482e-05],
          [ 0.0000e+00, -1.4708e-02,  0.0000e+00,  ...,  2.1495e-03,
            0.0000e+00, -3.1923e-04],
          [ 0.0000e+00, -3.9129e-03,  0.0000e+00,  ...,  8.8740e-04,
            0.0000e+00,  1.8688e-04]],

         [[ 2.2094e-26, -6.0553e-03,  0.0000e+00,  ...,  1.4373e-03,
            0.0000e+00,  4.3371e-04],
          [ 0.0000e+00, -4.8708e-03,  0.0000e+00,  ...,  1.5410e-03,
            0.0000e+00, -3.3060e-04],
          [ 0.0000e+00, -4.2963e-03,  0.0000e+00,  ...,  1.5011e-03,
            0.0000e+00, -3.1075e-04],
          ...,
          [ 0.0000e+00, -7.4514e-03,  0.0000e+00,  ...,  2.0611e-03,
            0.0000e+00, -1.2658e-04],
          [ 0.0000e+00, -1.3321e-02,  0.0000e+00,  ...,  2.0420e-03,
            0.0000e+00, -2.1078e-04],
          [ 0.0000e+00, -3.5022e-03,  0.0000e+00,  ...,  1.0584e-04,
            0.0000e+00,  3.8437e-04]]]], device='cuda:0'), True, 1
Epoch 0:   0%|          | 0/93 [00:03<?, ?it/s](DP) 
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from tools.cfg import py2cfg
import os
import torch
from torch import nn
import cv2
import numpy as np
import argparse
from pathlib import Path
from tools.metric import Evaluator
from pytorch_lightning.loggers import CSVLogger
import random


def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def get_args():
    parser = argparse.ArgumentParser()
    arg = parser.add_argument
    arg("-c", "--config_path", type=Path, help="Path to the config.", required=True)
    return parser.parse_args()


class Supervision_Train(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.net = config.net

        self.loss = config.loss

        self.metrics_train = Evaluator(num_class=config.num_classes)
        self.metrics_val = Evaluator(num_class=config.num_classes)

    def forward(self, x):
        # only net is used in the prediction/inference
        seg_pre = self.net(x)
        return seg_pre

    def training_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']

        prediction = self.net(img)
        loss = self.loss(prediction, mask)

        # 统一取主输出
        if isinstance(prediction, (list, tuple)):
            pred = prediction[0]   # shape: [B,C,H,W]
        else:
            pred = prediction      # shape: [B,C,H,W]

        pre_mask = torch.softmax(pred, dim=1)
        pre_mask = pre_mask.argmax(dim=1)  # [B,H,W]

        # debug一次
        if batch_idx == 0:
            print("pred shape:", pred.shape)
            print("pre_mask shape:", pre_mask.shape)
            print("mask shape:", mask.shape)

        for i in range(mask.shape[0]):
            self.metrics_train.add_batch(
                mask[i].cpu().numpy(),
                pre_mask[i].cpu().numpy()
            )

        return {"loss": loss}


    """   def training_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']

        prediction = self.net(img)
        loss = self.loss(prediction, mask)

        if self.config.use_aux_loss:
            pre_mask = nn.Softmax(dim=1)(prediction[0])
        else:
            pre_mask = nn.Softmax(dim=1)(prediction)

        pre_mask = pre_mask.argmax(dim=1)
        for i in range(mask.shape[0]):
            self.metrics_train.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())

        return {"loss": loss}"""

    def on_train_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_train.F1())

        OA = np.nanmean(self.metrics_train.OA())
        iou_per_class = self.metrics_train.Intersection_over_Union()
        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('train:', eval_value)

        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)
        self.metrics_train.reset()
        log_dict = {'train_mIoU': mIoU, 'train_F1': F1, 'train_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def validation_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']
        prediction = self.forward(img)
        pre_mask = nn.Softmax(dim=1)(prediction)
        pre_mask = pre_mask.argmax(dim=1)
        for i in range(mask.shape[0]):
            self.metrics_val.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())

        loss_val = self.loss(prediction, mask)
        return {"loss_val": loss_val}

    def on_validation_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_val.F1())

        OA = np.nanmean(self.metrics_val.OA())
        iou_per_class = self.metrics_val.Intersection_over_Union()

        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('val:', eval_value)
        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)

        self.metrics_val.reset()
        log_dict = {'val_mIoU': mIoU, 'val_F1': F1, 'val_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def configure_optimizers(self):
        optimizer = self.config.optimizer
        lr_scheduler = self.config.lr_scheduler

        return [optimizer], [lr_scheduler]

    def train_dataloader(self):

        return self.config.train_loader

    def val_dataloader(self):

        return self.config.val_loader


# training
def main():
    args = get_args()
    config = py2cfg(args.config_path)
    seed_everything(42)

    checkpoint_callback = ModelCheckpoint(save_top_k=config.save_top_k, monitor=config.monitor,
                                          save_last=config.save_last, mode=config.monitor_mode,
                                          dirpath=config.weights_path,
                                          filename=config.weights_name)
    logger = CSVLogger('lightning_logs', name=config.log_name)

    model = Supervision_Train(config)
    if config.pretrained_ckpt_path:
        model = Supervision_Train.load_from_checkpoint(config.pretrained_ckpt_path, config=config)

    trainer = pl.Trainer(
        devices=[0],  # 明确指定使用编号为0的GPU（单卡）
        accelerator="gpu",  # 强制使用GPU
        max_epochs=config.max_epoch,
        check_val_every_n_epoch=config.check_val_every_n_epoch,
        callbacks=[checkpoint_callback],
        logger=logger,
        # 完全移除strategy参数（单卡不需要分布式策略）
    )
    trainer.fit(model=model, ckpt_path=config.resume_ckpt_path)


if __name__ == "__main__":
   main()
val: {'mIoU': np.float64(0.04903625419123113), 'F1': np.float64(0.0942043117393754), 'OA': np.float64(0.24035147607324145)}
{'background': np.float64(0.0011351797573854532), 'building': np.float64(0.0003134133173133573), 'road': np.float64(0.06964601646560076), 'water': np.float64(0.0), 'barren': np.float64(0.004538710128489971), 'forest': np.float64(0.0011060856144562818), 'agricultural': np.float64(0.2665143740553721)}
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:534: Found 479 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.
Epoch 0:   0%|          | 0/315 [00:00<?, ?it/s]====== DEBUG SHAPE ======
img shape: torch.Size([8, 3, 512, 512])
mask shape: torch.Size([8, 512, 512])
prediction shape: torch.Size([8, 7, 512, 512])
=========================
pre_mask shape: torch.Size([7, 512])
Traceback (most recent call last):
  File "/home/linux/Desktop/DP-UNet-main/train_supervision.py", line 233, in <module>
    main()
  File "/home/linux/Desktop/DP-UNet-main/train_supervision.py", line 229, in main
    trainer.fit(model=model, ckpt_path=config.resume_ckpt_path)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 584, in fit
    call._call_and_handle_interrupt(
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 630, in _fit_impl
    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1079, in _run
    results = self._run_stage()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1123, in _run_stage
    self.fit_loop.run()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 217, in run
    self.advance()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 465, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.advance(data_fetcher)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 352, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1368, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/catalyst/contrib/nn/optimizers/lookahead.py", line 61, in step
    loss = self.optimizer.step(closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/adamw.py", line 197, in step
    loss = closure()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/train_supervision.py", line 83, in training_step
    self.metrics_train.add_batch(
  File "/home/linux/Desktop/DP-UNet-main/tools/metric.py", line 67, in add_batch
    assert gt_image.shape == pre_image.shape, 'pre_image shape {}, gt_image shape {}'.format(pre_image.shape,
AssertionError: pre_image shape (512,), gt_image shape (512, 512)
Epoch 0:   0%|          | 0/315 [00:02<?, ?it/s]





import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from tools.cfg import py2cfg
import os
import torch
from torch import nn
import cv2
import numpy as np
import argparse
from pathlib import Path
from tools.metric import Evaluator
from pytorch_lightning.loggers import CSVLogger
import random


def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def get_args():
    parser = argparse.ArgumentParser()
    arg = parser.add_argument
    arg("-c", "--config_path", type=Path, help="Path to the config.", required=True)
    return parser.parse_args()


class Supervision_Train(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.net = config.net
        self.loss = config.loss
        self.metrics_train = Evaluator(num_class=config.num_classes)
        self.metrics_val = Evaluator(num_class=config.num_classes)

    def forward(self, x):
        # only net is used in the prediction/inference
        seg_pre = self.net(x)
        return seg_pre

    def _get_pred(self, prediction):
        """
        统一处理模型输出，兼容单输出和多输出（aux_loss）情况
        Args:
            prediction: 模型输出，可能是 Tensor 或 (list, tuple)
        Returns:
            pred: 主输出 Tensor [B, C, H, W]
        """
        if isinstance(prediction, (list, tuple)):
            pred = prediction[0]
        else:
            pred = prediction
        return pred

    def training_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']

        # 前向传播
        prediction = self.net(img)
        
        # 计算损失（loss函数内部会处理多输出情况）
        loss = self.loss(prediction, mask)

        # 获取主输出用于计算指标
        pred = self._get_pred(prediction)
        
        # 获取预测类别 [B, H, W]
        # 注意：softmax不影响argmax结果，但为了与原代码保持一致，保留softmax
        pre_mask = nn.Softmax(dim=1)(pred)
        pre_mask = pre_mask.argmax(dim=1)

        # debug信息（仅第一个epoch的第一个batch打印）
        if batch_idx == 0 and self.current_epoch == 0:
            print("====== DEBUG TRAINING ======")
            print("img shape:", img.shape)
            print("mask shape:", mask.shape)
            print("prediction type:", type(prediction))
            if isinstance(prediction, (list, tuple)):
                print("prediction[0] shape:", prediction[0].shape)
            else:
                print("prediction shape:", prediction.shape)
            print("pred shape:", pred.shape)
            print("pre_mask shape:", pre_mask.shape)
            print("============================")

        # 计算训练指标
        for i in range(mask.shape[0]):
            self.metrics_train.add_batch(
                mask[i].cpu().numpy(),
                pre_mask[i].cpu().numpy()
            )

        return {"loss": loss}

    def on_train_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_train.F1())

        OA = np.nanmean(self.metrics_train.OA())
        iou_per_class = self.metrics_train.Intersection_over_Union()
        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('train:', eval_value)

        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)
        self.metrics_train.reset()
        log_dict = {'train_mIoU': mIoU, 'train_F1': F1, 'train_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def validation_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']
        
        # 前向传播
        prediction = self.forward(img)
        
        # 获取主输出用于计算指标
        pred = self._get_pred(prediction)
        
        # 获取预测类别 [B, H, W]
        pre_mask = nn.Softmax(dim=1)(pred)
        pre_mask = pre_mask.argmax(dim=1)

        # debug信息（仅第一个epoch的第一个batch打印）
        if batch_idx == 0 and self.current_epoch == 0:
            print("====== DEBUG VALIDATION ======")
            print("pred shape:", pred.shape)
            print("pre_mask shape:", pre_mask.shape)
            print("mask shape:", mask.shape)
            print("==============================")

        # 计算验证指标
        for i in range(mask.shape[0]):
            self.metrics_val.add_batch(
                mask[i].cpu().numpy(),
                pre_mask[i].cpu().numpy()
            )

        # 计算验证损失
        loss_val = self.loss(prediction, mask)
        return {"loss_val": loss_val}

    def on_validation_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_val.F1())

        OA = np.nanmean(self.metrics_val.OA())
        iou_per_class = self.metrics_val.Intersection_over_Union()

        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('val:', eval_value)
        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)

        self.metrics_val.reset()
        log_dict = {'val_mIoU': mIoU, 'val_F1': F1, 'val_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def configure_optimizers(self):
        optimizer = self.config.optimizer
        lr_scheduler = self.config.lr_scheduler
        return [optimizer], [lr_scheduler]

    def train_dataloader(self):
        return self.config.train_loader

    def val_dataloader(self):
        return self.config.val_loader


# training
def main():
    args = get_args()
    config = py2cfg(args.config_path)
    seed_everything(42)

    checkpoint_callback = ModelCheckpoint(
        save_top_k=config.save_top_k,
        monitor=config.monitor,
        save_last=config.save_last,
        mode=config.monitor_mode,
        dirpath=config.weights_path,
        filename=config.weights_name
    )
    logger = CSVLogger('lightning_logs', name=config.log_name)

    model = Supervision_Train(config)
    if config.pretrained_ckpt_path:
        model = Supervision_Train.load_from_checkpoint(
            config.pretrained_ckpt_path,
            config=config
        )

    trainer = pl.Trainer(
        devices=[0],
        accelerator="gpu",
        max_epochs=config.max_epoch,
        check_val_every_n_epoch=config.check_val_every_n_epoch,
        callbacks=[checkpoint_callback],
        logger=logger,
    )
    trainer.fit(model=model, ckpt_path=config.resume_ckpt_path)


if __name__ == "__main__":
    main()

(DP) linux@ubuntu-4356:~/Desktop/DP-UNet-main$ bash test_loveda.sh
test_loveda.sh
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:159: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:228: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, dout):
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:296: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, dout):
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:441: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:521: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, dout):
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/triton/layernorm.py:509: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/mamba_ssm/ops/triton/layernorm.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, dout, *args):
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name swsl_resnet18 to current resnet18.fb_swsl_ig1b_ft_in1k.
  model = create_fn(
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/linux/Desktop/DP-UNet-main/GeoSeg/models/RS3Mamba.py:577: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(ckpt_path, map_location='cpu')
Unsupported operator aten::silu encountered 30 time(s)
Unsupported operator aten::mul encountered 97 time(s)
Unsupported operator aten::flip encountered 30 time(s)
Unsupported operator aten::exp encountered 15 time(s)
Unsupported operator aten::neg encountered 15 time(s)
Unsupported operator prim::PythonOp.SelectiveScanFn encountered 15 time(s)
Unsupported operator aten::add encountered 103 time(s)
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 15 time(s)
Unsupported operator aten::mul_ encountered 35 time(s)
Unsupported operator aten::softmax encountered 7 time(s)
Unsupported operator aten::pad encountered 21 time(s)
Unsupported operator aten::avg_pool2d encountered 14 time(s)
Unsupported operator aten::hardtanh encountered 13 time(s)
Unsupported operator aten::dropout_ encountered 14 time(s)
Unsupported operator aten::sum encountered 3 time(s)
Unsupported operator aten::div encountered 3 time(s)
Unsupported operator aten::sigmoid encountered 2 time(s)
Unsupported operator aten::feature_dropout_ encountered 1 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
vssm_encoder.layers.0.blocks.0.drop_path, vssm_encoder.layers.0.blocks.1.drop_path, vssm_encoder.layers.1.blocks.0.drop_path, vssm_encoder.layers.1.blocks.1.drop_path, vssm_encoder.layers.2.blocks.0.drop_path, vssm_encoder.layers.2.blocks.1.drop_path, vssm_encoder.layers.2.blocks.2.drop_path, vssm_encoder.layers.2.blocks.3.drop_path, vssm_encoder.layers.2.blocks.4.drop_path, vssm_encoder.layers.2.blocks.5.drop_path, vssm_encoder.layers.2.blocks.6.drop_path, vssm_encoder.layers.2.blocks.7.drop_path, vssm_encoder.layers.2.blocks.8.drop_path, vssm_encoder.layers.3.blocks.0.drop_path, vssm_encoder.layers.3.blocks.1.drop_path
/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
=> merge config from /home/linux/Desktop/DP-UNet-main/config/loveda/vmamba_tiny.yaml
AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 24
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: 
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: True
  ZIP_MODE: False
EVAL_MODE: False
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: vssm_tiny
  NUM_CLASSES: 1000
  PRETRAIN_CKPT: /home/linux/Desktop/DP-UNet-main/vmamba_tiny_e292.pth
  RESUME: 
  SWIN:
    APE: False
    DECODER_DEPTHS: [2, 2, 6, 2]
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    FINAL_UPSAMPLE: expand_first
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 4
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  TYPE: vssm
  VSSM:
    DEPTHS: [2, 2, 2, 2]
    EMBED_DIM: 128
OUTPUT: 
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: True
  BASE_LR: 0.0005
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 5e-06
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 5e-07
  WEIGHT_DECAY: 0.05
Loading weights from: /home/linux/Desktop/DP-UNet-main/vmamba_tiny_e292.pth
Use raw state_dict
Skipping unmatched key: model
Skipping unmatched key: optimizer
Skipping unmatched key: lr_scheduler
Skipping unmatched key: max_accuracy
Skipping unmatched key: scaler
Skipping unmatched key: epoch
Skipping unmatched key: config
Load pretrained weights done!
Total trainable parameters: 43324958 (43.32M)
FLOPs (G): 9.868476376
Params (M): 323.735216
average time: 0.03837894296646118
average fps: 26.055954716467465
fastest time: 0.03479361534118652
fastest fps: 28.740905197519442
slowest time: 0.14574694633483887
slowest fps: 6.861207216668548
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 418/418 [30:46<00:00,  4.42s/it]
F1_background:0.6243095351647958, IOU_background:0.45381541205897846
F1_building:0.4971108945717761, IOU_building:0.3307701764396864
F1_road:0.5522825270283175, IOU_road:0.38148501854762124
F1_water:0.6646429562509254, IOU_water:0.49772677604254095
F1_barren:0.11390862015391012, IOU_barren:0.06039400920394715
F1_forest:0.5200206300105628, IOU_forest:0.3513701883657164
F1_agricultural:0.597284380342883, IOU_agricultural:0.42580575276468696
F1:0.5099370776461672, mIOU:0.3573381904890254, OA:0.5837804981033137
images writing spends: 55.78136134147644 s











