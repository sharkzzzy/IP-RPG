1. CSMamba 回顾段落可精简（节省约 2 行）
原文：

To clearly articulate the motivation for our improvements, we begin by revisiting the decoder CSMamba component of the baseline model. As illustrated in Fig. 1(b), its central component is the two-dimensional state space mechanism (2D-SSM), which captures global spatial context through a four-directional scanning process, please refer to [5] for more details about it. The SS2D module internally integrates both channel and spatial attention mechanisms as gates to enhance its feature selection capabilities.

建议改为：

To motivate our improvements, we revisit the baseline CSMamba decoder. As shown in Fig. 1(b), its core SS2D module captures global context via four-directional 2D-SSM scanning [5], with internal channel and spatial attention gates for feature selection.

2. APFG 末尾的 SiLU/GeLU 解释可删除（节省约 2 行）
原文（R1-3 已满足，但可以精简）：

We adopt SiLU in SS2D following the original VMamba [5] design for architectural consistency, while using GeLU in APFG for its smoother gradient profile suited to fusing features from heterogeneous paths.

建议：删除整句（审稿人只要求 justify，你已经在正文提到了选择，可以移到 footnote 或直接删除，因为这不是核心贡献）

如果担心审稿人追问，可改为更短的版本：

We use SiLU in SS2D for consistency with VMamba [5], and GeLU in APFG for smoother gradients.

二、II-B 部分（MSK 模块）
3. 效率对比段落有冗余（节省约 3 行）
当前有重复内容：

第一段末尾：

This design improves efficiency compared to the baseline MSAA by operating in a compressed feature space and using only spatial attention. A standard channel attention module requires C'²/2 parameters, while the spatial attention here uses only 98 parameters (a 7×7 conv with 2 input channels). When C'=32, this reduces attention-related parameters from 512 to 98, achieving over 80% reduction.

建议精简为一句：

This design improves efficiency over MSAA by operating in compressed space (C/4 channels) with spatial-only attention, reducing attention parameters by over 80% (98 vs. 512 when C'=32).

三、III-A 部分（Datasets and Implementation）
4. 数据集描述可压缩（节省约 2 行）
原文：

The ISPRS Potsdam dataset consists of 23 images (excluding image 710 due to erroneous annotations) for training and 14 for testing. The ISPRS Vaihingen dataset includes 12 training blocks and 4 testing blocks. The LoveDA dataset contains 1,156 training images and 677 test images.

建议改为：

ISPRS Potsdam contains 23/14 images for training/testing (excluding erroneous image 710); Vaihingen has 12/4 blocks; LoveDA has 1,156/677 images.

5. 实现细节可压缩（节省约 2 行）
原文：

All experiments were implemented in PyTorch on an NVIDIA RTX 3090 GPU. We used the AdamW optimizer with an initial learning rate of 6e-4 and a cosine annealing schedule. For training, input images were randomly cropped to 512×512 patches and augmented through random scaling (0.5-1.5), flipping, rotation, and brightness/contrast adjustment (limit=0.25, p=0.25). All inputs were normalized. During testing, we used the original 1024×1024 resolution and applied test-time flipping for accuracy.

建议改为：

Experiments used PyTorch on an RTX 3090 GPU with AdamW optimizer (lr=6e-4, cosine schedule). Training used 512×512 crops with standard augmentations (scaling, flipping, rotation, color jittering). Testing used 1024×1024 resolution with test-time flipping.

四、III-B 部分（Performance Comparison）
6. 性能描述可大幅压缩（节省约 4 行）
原文：

To validate its effectiveness, we conducted comparisons with state-of-the-art methods on three public datasets. DP-UNet achieved state-of-the-art performance across all benchmarks, demonstrating strong generalization capabilities. As detailed in Table I-III: DP-UNet achieved state-of-the-art performance among the compared methods across all benchmarks. On ISPRS Potsdam (86.71% mIoU), it excelled in boundary-sensitive categories like "Lowveg" and "Building", yielding clearer contours (Fig. 4). On ISPRS Vaihingen (83.84% mIoU), it showed strong performance on small objects like "Cars" (Fig. 5), benefiting from the dedicated local-detail enhancement path (including PMC). On LoveDA (53.21% mIoU), it improved structured categories like "Building" and "Road" (Fig. 6), highlighting effective global–local collaboration.

问题： 第一句和第二句重复说了两遍 "state-of-the-art"

建议改为：

As shown in Tables I-III, DP-UNet achieves state-of-the-art mIoU on all benchmarks: 86.71% on Potsdam with clearer boundaries (Fig. 4), 83.84% on Vaihingen with better small-object recognition (Fig. 5), and 53.21% on LoveDA with improved building/road segmentation (Fig. 6), demonstrating effective global–local collaboration.

五、III-C 部分（Further Analysis）
7. 开头句可删除（节省约 1 行）
原文：

Comprehensive experiments conducted on the LoveDA dataset using 1024×1024 images evaluate both the effectiveness and efficiency of our method.

建议：直接删除，因为下面的内容已经说明了这一点。

六、表格优化（可选，节省显著空间）
8. 合并 Table III 和 Table V（节省约 0.2 页）
你已经做了这个修改（在 Table III 中加入了 FLOPs/FPS/Param 列）。

如果空间仍不够，可以考虑：

删除 Table V，因为信息已在 Table III 中
或者只保留 Table V 中与 baseline 的对比数据
