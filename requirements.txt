Section II 开头（完整版）
The proposed DP-UNet is an improved architecture over CM-UNet, designed to enhance segmentation accuracy and optimize computational efficiency through structural innovations. It achieves these goals by replacing the original MSAA with the MSK module to eliminate redundant computations and by introducing the DVSS block in place of the single-path CSMamba block, thereby decoupling global context modeling from local detail refinement. As illustrated in Fig. 1(a), the network consists of three main components: a ResNet-18-based [6] encoder for multi-level feature extraction, MSK modules for multi-scale feature fusion, and a DVSS-based decoder for progressive upsampling and refinement.

Given an input image I ∈ ℝ^(H×W×3), the encoder produces four-level features {F₁, F₂, F₃, F₄} at 1/4, 1/8, 1/16, and 1/32 of the original resolution. Three MSK modules enhance {F₁, F₂, F₃} through cross-scale fusion to obtain enriched skip features {F₁', F₂', F₃'} (Section II-B), while F₄ is preserved. The decoder reconstructs the segmentation map through four stages: stage 0 processes F₄, and stages 1–3 progressively fuse with {F₃', F₂', F₁'} via skip connections and DVSS blocks (Section II-A). The final prediction P is produced by a 1×1 convolution head and bilinearly upsampled to the original resolution. Formally:

{
F
1
,
F
2
,
F
3
,
F
4
}
=
E
n
c
(
I
)
,
{
F
1
′
,
F
2
′
,
F
3
′
}
=
M
S
K
(
{
F
1
,
F
2
,
F
3
}
)
{F 
1
​
 ,F 
2
​
 ,F 
3
​
 ,F 
4
​
 }=Enc(I),{F 
1
′
​
 ,F 
2
′
​
 ,F 
3
′
​
 }=MSK({F 
1
​
 ,F 
2
​
 ,F 
3
​
 })

D
0
=
D
V
S
S
0
(
F
4
)
,
D
i
=
D
V
S
S
i
(
P
r
o
j
(
[
U
p
(
D
i
−
1
)
,
F
4
−
i
′
]
)
)
,
  
i
=
1
,
2
,
3
D 
0
​
 =DVSS 
0
​
 (F 
4
​
 ),D 
i
​
 =DVSS 
i
​
 (Proj([Up(D 
i−1
​
 ),F 
4−i
′
​
 ])),i=1,2,3

P
=
H
e
a
d
(
D
3
)
,
L
=
C
E
(
P
,
Y
)
+
∑
i
=
1
3
λ
i
⋅
C
E
(
P
(
i
)
,
Y
)
P=Head(D 
3
​
 ),L=CE(P,Y)+∑ 
i=1
3
​
 λ 
i
​
 ⋅CE(P 
(i)
 ,Y)

where {P^(i)} are auxiliary predictions from decoder stages 1–3, and λᵢ = 0.4, 0.3, 0.2 correspond to stages 3, 2, 1 respectively. Within each DVSS block, features are processed through parallel global and local paths—the latter applying learned weight modulation W_eff (Eq. 1) to enhance fine details—before being fused by the APFG module (detailed in Section II-A).

Section II-A（完整版）
A. Main Decoder (DVSS)

The decoder consists of four stages. At stage 0, F₄ is refined by a DVSS block and upsampled. For stages i ∈ {1, 2, 3}, the upsampled feature is concatenated with the corresponding enriched skip feature F'₄₋ᵢ, projected by a 1×1 convolution, and refined by a DVSS block. Auxiliary heads attached to stages 1–3 provide deep supervision.

To clearly articulate the motivation for our improvements, we begin by revisiting the decoder CSMamba component of the baseline model. As illustrated in Fig. 1(b), its central component is the two-dimensional state space mechanism (2D-SSM), which captures global spatial context through a four-directional scanning process, please refer to [5] for more details about it. The SS2D module internally integrates both channel and spatial attention mechanisms as gates to enhance its feature selection capabilities. We posit that global semantic context modeling and local spatial detail refinement are distinct tasks, yet they share the same underlying feature foundation. Processing them sequentially within the single-path architecture of the original decoder may prevent either task from achieving optimal performance. The DVSS adopts a "shared base, separate enhancement" design philosophy. Its core idea is to have global and local processing share the same underlying feature source before undergoing task-specific optimization.

The structure of the DVSS is shown in Fig. 1(c). The input feature X ∈ ℝ^(B×H×W×C) is first normalized and processed by the SS2D module to produce a shared foundational feature F_s. This feature serves both paths: the global path retains F_s directly, while the local detail path processes it as follows. It undergoes channel recalibration through an Efficient Channel Attention (ECA) [7] module, which models cross-channel dependencies via a lightweight 1D convolution. This recalibration helps the subsequent PMC focus on informative channels. The recalibrated feature is enhanced via a residual connection before being fed into the PMC module.

The PMC module is inspired by Learnable Deformable Convolution (LDC) [8], but operates in the parameter domain rather than the spatial domain. This design choice is motivated by a key observation: standard convolutions exhibit a center-dominance tendency, where center weights consistently receive larger magnitudes after training. This occurs partly because the kernel center always corresponds to a well-aligned sampling position across spatial locations, and is repeatedly reinforced by overlapping receptive fields, leading to more stable gradient signals and larger cumulative updates during optimization. Although a sufficiently expressive network could, in principle, learn non-center-dominant kernels from data, standard training does not explicitly encourage such behavior; consequently, gradient-based optimization tends to converge to center-reliant solutions that minimize training loss but remain suboptimal for fine-detail extraction. By shifting the deformation from the spatial to the parameter domain, PMC eliminates the complex coordinate offset and interpolation computations required in traditional LDC, while introducing a lightweight, learnable inhibitory prior to counteract center-dominance.

The core mechanism works as follows: a dynamic mask, correlated with the convolutional kernel weights, adaptively modulates the convolution weights and suppresses the intensity at the kernel's center. This suppression compels the network to gather information from neighboring positions, enhancing sensitivity to edges and fine textures—precisely the local details this path aims to capture.

As shown in Fig. 2(a), the weight tensor W of the standard convolution kernel is first summed across the spatial dimensions to generate a channel strength matrix S, which characterizes the importance of each input-output channel pair. Then, a dynamic mask M_d is generated by multiplying the learnable mask matrix W_m, the fixed center mask M_c (with a center value of 1 and zeros elsewhere), and the previously mentioned strength matrix S. A learnable scaling factor θ is then introduced, and the original convolution weights are modulated using the expression in Eq. (1), yielding the effective convolution weights W_eff.

W
e
f
f
=
W
⊙
(
B
−
θ
×
M
d
)
(
1
)
W 
eff
​
 =W⊙(B−θ×M 
d
​
 )(1)

where B is a tensor of all ones, and ⊙ represents element-wise multiplication. These modulated weights W_eff are applied to convolve the feature (ECA(F_s) + F_s), producing the local path output F_l.

Finally, the outputs of both paths are fused via the APFG module. As shown in Fig. 2(b), APFG applies channel-wise attention to each path independently: it performs global average pooling, followed by a bottleneck structure (reduction and expansion via linear layers) with sigmoid activation to generate attention weights. Each path's feature is recalibrated by its own attention weights, and the results are summed: Y = X + DropPath(APFG(F_g) + APFG(F_l)). We adopt SiLU in SS2D following the original VMamba [5] design for architectural consistency, while using GeLU in APFG for its smoother gradient profile suited to fusing features from heterogeneous paths.

Section II-B（完整版）
B. Multi-Scale Fusion Module (MSK)

In the feature fusion stage, we reconsider the role of attention mechanisms and propose the more computationally efficient MSK module. This design is motivated by two key observations. First, the ECA module within the DVSS block's local detail path already accounts for inter-channel dependencies, making additional channel attention at fusion redundant. Second, the core challenge during this stage is bridging the semantic gap and spatial misalignment across multi-resolution feature maps, where integrating spatial information is far more critical than recalibrating channels. Thus, we discard complex hybrid attention in favor of a spatial-only feature fusion strategy.

Three MSK modules operate at different encoder scales to generate {F₁', F₂', F₃'}. For each MSK, the other two scales are first resized via bilinear interpolation to match the target resolution, then concatenated with the target feature as input. As illustrated in Fig. 3, the concatenated features are first compressed by a 1×1 convolution to C/4 channels. In this compressed space, three standard convolutions (3×3, 5×5, 7×7) are applied in parallel to extract multi-scale spatial patterns, and their outputs are summed together. Although these are regular convolutions, the overall cost remains low because all operations are performed on the compressed feature map with only C/4 channels. For spatial attention, average and max pooling are applied along the channel axis, and their results are concatenated and processed by a 7×7 convolution with sigmoid activation, producing an attention map that modulates the fused features via multiplication. A final 1×1 convolution restores the output to the target channel dimension.

This design improves efficiency compared to the baseline MSAA by operating in a compressed feature space and using only spatial attention. A standard channel attention module requires C'²/2 parameters, while the spatial attention here uses only 98 parameters (a 7×7 conv with 2 input channels). When C'=32, this reduces attention-related parameters from 512 to 98, achieving over 80% reduction.

