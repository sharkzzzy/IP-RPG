Abstract—Remote sensing image segmentation has long struggled with balancing efficiency and accuracy. Most current research focuses on hybrid architectures that combine state-space models with CNNs, with CM-UNet being a notable example. However, its single-path decoding mechanism creates a performance bottleneck, and its multi-level homogeneous attention mechanism introduces significant computational redundancy. This paper proposes a novel Dual-Path U-Net (DP-UNet), using CM-UNet as the baseline. Our key insight is to explicitly decouple the processing of global context and local details, which are inherently distinct tasks. First, we innovate by designing a Decoupled Visual State Space (DVSS) block: one path models long-range dependencies through a simplified state-space model, while the other path integrates Efficient Channel Attention (ECA) and a new Parameter-Modulated Convolution (PMC) module to enhance detail extraction. The features from both paths are fused through an Adaptive Path Fusion Gate (APFG) module. Second, we streamline attention by removing redundant units from the SS2D module in CM-UNet and simplifying its multi-scale fusion module into a Multi-scale Spatial Kernel (MSK) module, which refines its structure to focus solely on spatial attention, reducing model complexity. Experimental results on three benchmarks show that DP-UNet outperforms existing state-of-the-art methods in both accuracy and efficiency. This study demonstrates the potential of explicit decoupling architectures in visual tasks. The codes are available at https://github.com/sharkzzzy/DP-UNet.

Index Terms—Remote Sensing, Semantic Segmentation, Visual State space model

I. Introduction
Remote sensing image segmentation is extensively applied in various fields such as urban planning, environmental monitoring, and disaster assessment. Due to the high resolution, multi-scale objects, multiple categories, and complex backgrounds typical of remote sensing images, how to effectively capture long-range dependencies, integrate multi-scale information, and maintain computational efficiency has become a major focus of current research.
To address these challenges, most current research combines lightweight state-space models with CNN-based architectures. This trend is driven by the limitations of traditional U-Net [1] 

architectures, which, whether based on CNNs [2] or Transformers [3], often struggle with large-scale scenes that contain significant variations in objects, making it difficult to fully capture global context. Additionally, they may suffer from high computational complexity. Combining state-space models with CNNs is motivated not only by the need for efficiency but also by the ability to fully leverage the rich features extracted by well-pretrained CNN models, enhancing both context understanding and segmentation performance. A notable advancement is CM-UNet [4], which skillfully combines a CNN-based encoder with a Mamba-based [5] decoder to effectively extract local features and integrate global contextual information. Specifically, CM-UNet introduces a CSMamba block in the decoding part, which adopts the core idea of Mamba and builds upon the original attention mechanisms by employing channel and spatial attention mechanisms as gating conditions, thereby strengthening feature interaction. Additionally, the model adopts a Multi-Scale Attention Aggregation (MSAA) module to merge features across different scales.
Despite its strong performance across various tasks, CM-UNet still exhibits certain limitations. First, its coupled architecture presents a bottleneck. The CSMamba module processes both global context modeling and local detail refinement within a single sequential pathway. We argue that these are two fundamentally distinct tasks requiring specialized processing. Forcing them to share parameters and computational flow limits the model's capacity to excel at either task optimally. Second, it suffers from attention redundancy. The MSAA module at the feature fusion stage already incorporates both channel and spatial attention. The subsequent reapplication of the same channel and spatial attention mechanisms within the SS2D module. Although this multi-level homogeneous attention design enhances feature focusing to some extent, it inevitably increases model complexity and computational overhead.
To address these issues, we propose DP-UNet, a novel architecture designed for enhanced computational efficiency and segmentation performance. Our approach is guided by two core principles: decoupling and streamlining. First, we introduce a DVSS block to replace the single-branch design, decoupling processing into parallel global context and local detail paths, later fused by an APFG module. Second, we streamline attention by removing redundancies within SS2D and simplifying the multi-scale fusion into a lightweight MSK module, which focuses solely on spatial attention. The contributions of this work are summarized as follows:
 
 
	 (a) Overall architecture of DP-UNet. The red arrows indicating upsampling and blue arrows indicating downsampling. (b) The baseline CSMamba block. (c) Our proposed DVSS block. 

	We identify the limitation of the coupled single-branch design in the baseline. The proposed DVSS explicitly decouples global context modeling from local detail enhancement, allowing each sub-task to be optimized by a dedicated pathway, thereby unleashing the model’s full potential.
	We identify and eliminate dual attention redundancies in the original model. This streamlining strategy involves two key steps: removing the internal spatial and channel attention gating within the SS2D module, and modifying the feature fusion module into the lightweight MSK. These refinements substantially improve model efficiency without sacrificing performance.
	Extensive experiments on three widely used public RS datasets—ISPRS Potsdam, Vaihingen, and LoveDA—demonstrate the superiority of the proposed DP-UNet. 
II. Methodology
The proposed DP-UNet is an improved architecture over CM-UNet, designed to enhance segmentation accuracy and optimize computational efficiency through structural innovations. It achieves these goals by replacing the original MSAA with the MSK module to eliminate redundant computations and by introducing the DVSS block in place of the single-path CSMamba block, thereby decoupling global context modeling from local detail refinement. As illustrated in Fig. 1(a), the network consists of three main components: the ResNet-18-based [6] encoders for extracting multi-level features, the MSK modules for multi-scale feature fusion, and the decoders with DVSS block that utilize a dual-path approach for better semantic understanding.
Specifically, the encoder produces multi-level features F_1,\ F_2,\ F_3,\ F_4 from the input image, and MSK fuses F_1,F_2,F_3 (see Section II-B). Each DVSS block processes its input X through: (1) shared feature extractionF_s=SS2D\left(LN\left(X\right)\right); (2) global path F_g=F_s; (3) local path F_l=Conv\left(ECA\left(F_s\right)+F_s;W_{eff}\right), where W_{eff} from Eq.(1) is applied to ECA-recalibrated features; (4) fusion Y=X+DropPath\left(APFG\left(F_g,F_l\right)\right). The model is trained with L=L_{main}+\sum_{i=1}^{3}\lambda_i\cdot L_{aux}^{\left(i\right)}, where all terms are cross-entropy losses and \lambda_i = 0.4, 0.3, 0.2.
A. Main decoder (DVSS)
To clearly articulate the motivation for our improvements, we begin by revisiting the decoder CSMamba component of 
the baseline model. As illustrated in Fig. 1(b), its central component is the two-dimensional state space mechanism (2D-SSM), which captures global spatial context through a four-directional scanning process, please refer to [5] for more details about it. The SS2D module internally integrates both channel and spatial attention mechanisms as gates to enhance its feature selection capabilities. We posit that global semantic context modeling and local spatial detail refinement are distinct tasks, yet they share the same underlying feature foundation. Processing them sequentially within the single-path architecture of the original decoder may prevent either task from achieving optimal performance. The DVSS adopts a "shared base, separate enhancement" design philosophy. Its core idea is to have global and local processing share the same underlying feature source before undergoing task-specific optimization. 
The structure of the DVSS is shown in Fig. 1(c). The input feature X\in\mathbb{R}^{B\times H\times W\times C} is first normalized and processed by the SS2D module to produce a shared foundational feature. The local detail path takes this feature as input. It undergoes channel recalibration through an Efficient Channel Attention (ECA) [7] module, which models cross-channel dependencies via a lightweight 1D convolution. This recalibration helps the subsequent PMC focus on informative channels. The recalibrated feature is enhanced via a residual connection before being fed into the PMC module.
The PMC module is inspired by Learnable Deformable Convolution (LDC) [8] and aims to enhance the representational capacity of standard convolution through a dynamic inhibitory masking mechanism. Its core idea is to utilize a dynamic mask, correlated with the convolutional kernel weights, to adaptively modulate the convolution weights and suppress the intensity at the kernel's center. This suppression acts not as a simple weakening but as an efficient regularization method, compelling the network to learn richer and more dispersed feature representations. By shifting the deformation from the spatial to the parameter domain, this design eliminates the complex coordinate offset and interpolation computations required in traditional LDC. The approach reduces over-reliance on the central position common  in  standard  convolution,  enhances  perception  of
 
   
	 (a) The workflow of the PMC module. (b) The structure of the APFG module. 
 
subtle local structural variations, and achieves efficient and concise detail enhancement.
As shown in Fig. 2(a), the weight tensor W, of the standard convolution kernel is first summed across the spatial dimensions to generate a channel strength matrix S, which characterizes the importance of each input-output channel pair. Then, a dynamic mask M_d is generated by multiplying the learnable mask matrix W_m, the fixed center mask M_c (with a center value of 1 and zeros elsewhere), and the previously mentioned strength matrix S. A learnable scaling factor \theta is then introduced, and the original convolution weights are modulated using the expression in Eq. (1), yielding the effective convolution weights W_{eff}. 
Weff=W⊙B-θ×Md#1
where B is a tensor of all ones, and \odot represents element-wise multiplication. These modulated weights are used for the convolution operation on the input feature from ECA.
A natural question is why PMC is necessary rather than letting the network learn such patterns implicitly. Standard convolutions exhibit a center-dominance tendency, where center weights receive larger magnitudes after training. This is an architectural bias that data-driven optimization reinforces rather than corrects. PMC provides an explicit prior to counteract this. Intuitively, suppressing center weights forces the network to gather information from neighboring positions, enhancing sensitivity to edges and fine textures—precisely the local details this path aims to capture.
Finally, the outputs of the global and local paths are fused by the APFG module. As shown in Fig. 2(b), the APFG first applies global average pooling to generate channel-wise attention weights, which are used to recalibrate features from both paths via a weighted attention mechanism. The integrated result is further refined through a residual connection, with DropPath applied as a regularization strategy during training to produce the final output. We adopt SiLU in SS2D following the original VMamba [5] design for architectural consistency, while using GeLU in APFG for its smoother gradient profile suited to fusing features from heterogeneous paths.

B. Multi-Scale Fusion Module (MSK)
In the feature fusion stage, we reconsider the role of attention mechanisms and propose the more computationally efficient MSK module. This design is motivated by two key observations. First, the ECA module within the DVSS block’s local detail path already accounts for inter-channel dependencies, making additional channel attention at fusion redundant. Second, the core challenge during this stage is bridging the semantic gap and spatial misalignment across multi-resolution   feature   maps,   where   integrating   spatial 
 
	The structure of the MSK module.

information is far more critical than recalibrating channels. Thus, we discard complex hybrid attention in favor of a spatial-only feature fusion strategy. 
The MSK module integrates multi-scale encoder features through an efficient spatial-focused fusion process. As illustrated in Fig. 3, it takes the channel-wise concatenation of encoder features (F1,\ F2,\ F3) as input. A 1\times1 convolution first compresses the features to C/4 channels. In this compressed space, three parallel convolutions\left(3\times3,\ 5\times5,\ 7\times7\right) extract multi-scale spatial patterns, and their outputs are summed together. For spatial attention, average and max pooling are applied along the channel axis, and their results are concatenated and processed by a 7\times7 convolution with sigmoid activation, producing an attention map that modulates the fused features via multiplication. A final 1\times1 convolution restores the channel dimension.
This design improves efficiency by operating in a compressed feature space and using only spatial attention. A standard channel attention module requires C^{\prime2}/2 parameters, while the spatial attention here uses only 98 parameters (a\ 7\times7 conv with 2 input channels). When C^\prime=32, this reduces attention-related parameters from ~512 to 98, achieving over 80% reduction compared to the baseline MSAA.

III. Results
A. Datasets and Implementation Details
We evaluated the proposed DP-UNet on three public remote sensing image segmentation datasets: LoveDA [9], ISPRS Potsdam,  and  ISPRS Vaihingen.  The ISPRS Potsdam dataset
 
TABLE I
EXPERIMENTAL RESULTS ON ISPRS POTSDAM DATASET
Method	Backbone	Imp.surf.	Building	Lowveg.	Tree	Car	mF1	OA	mIoU
DANet [10]
R18	91.02	95.63	86.14	87.64	84.32	88.92	89.12	80.38
ABCNet [11]
R18	93.28	95.91	86.78	87.81	95.49	91.85	90.51	85.17
Segmenter [12]
ViT-T	91.56	95.38	85.43	85.01	88.51	89.27	88.78	80.71
BANet [13]
ResT-L	93.36	96.70	87.48	89.11	96.05	92.55	91.01	86.36
UNetFormer[14]
R18	93.69	96.10	87.27	88.72	96.58	92.47	91.07	86.23
CM-UNet [4]
R18	93.62	96.49	87.42	88.51	96.09	92.43	91.04	86.15
DP-UNet	R18	94.10	96.70	87.78	89.13	96.09	92.76	91.57	86.71

 	 	 	 
 	 	 	 
(a)	(b)	(c)	(d)
 
	 Qualitative performance comparisons on the ISPRS Potsdam. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet. 

TABLE II
EXPERIMENTAL RESULTS ON ISPRRS VAIHINGEN DATASET
Method	Backbone	Imp.surf.	Building	Lowveg.	Tree	Car	mF1	OA	mIoU
DANet [10]
R18	90.07	93.95	82.21	87.31	44.52	79.67	88.24	69.46
ABCNet  [11]
R18	92.72	95.20	84.55	89.71	85.35	89.52	90.78	81.31
Segmenter  [12]
ViT-T	89.84	93.05	81.23	88.90	67.66	84.10	88.17	73.60
BANet  [13]
ResT-L	92.22	95.23	83.81	89.92	86.82	89.63	90.52	81.45
UNetFormer [14]
R18	93.07	95.42	85.03	90.74	88.99	90.65	91.21	83.09
CM-UNet  [4]
R18	93.09	95.75	85.02	90.70	89.86	90.89	91.26	83.49
DP-UNet	R18	93.18	95.88	85.37	90.84	90.22	91.10	91.43	83.84

 	 	 	 
 	 	 	 
(a)	(b)	(c)	(d)
 
	 Qualitative performance comparisons on the ISPRS Vaihingen. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet. 

consists of 23 images (excluding image 710 due to erroneous 
annotations) for training and 14 for testing. The ISPRS Vaihingen dataset includes 12 training blocks and 4 testing blocks. The LoveDA dataset contains 1,156 training images and 677 test images. The segmentation results were evaluated using several standard metrics: mean F1-score (mF1), mean Intersection over Union (mIoU), and Overall Accuracy (OA).
TABLE III
EXPERIMENTAL RESULTS ON LOVEDA DATASET
Method	Backbone	Background	Building	Road	Water	Barren	Forest	Agriculture	mIoU
DeepLabV3+[15]
R50	43.02	50.92	52.04	74.43	10.40	44.21	58.59	47.65
BANet [13]
ResT-L	43.71	51.55	51.11	76.98	16.68	44.31	62.12	49.62
Segmenter [12]
ViT-T	38.03	50.74	48.76	77.40	13.36	43.48	58.20	47.18
ABCNet [11]
ViT-R50	53.00	62.18	52.42	62.02	29.80	41.61	47.27	49.80
UNetFormer [14]
R18	53.44	56.76	51.48	64.48	34.20	39.51	48.20	49.72
CM-UNet [4]
R18	55.65	62.70	53.56	65.73	34.90	42.17	54.17	52.84
DP-UNet	R18	54.99	65.61	55.05	67.83	32.52	44.70	51.73	53.21

 	 	 	 
 	 	 	 
(a)	(b)	(c)	(d)
 
	 Qualitative performance comparisons on the Loveda. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet.

All experiments were implemented in PyTorch on an NVIDIA RTX 3090 GPU. We used the AdamW optimizer with an initial learning rate of 6e-4 and a cosine annealing schedule. For training, input images were randomly cropped to 512×512 patches and augmented through random scaling (0.5-1.5), flipping, rotation, and brightness/contrast adjustment (limit=0.25, p=0.25). All inputs were normalized. During testing, we used the original 1024×1024 resolution and applied test-time flipping for accuracy.

B. Performance Comparison
To validate its effectiveness, we conducted comparisons with state-of-the-art methods on three public datasets. DP-UNet achieved state-of-the-art performance across all benchmarks, demonstrating strong generalization capabilities. As detailed in Table I-III:
On ISPRS Potsdam (86.71% mIoU), it excelled in segmenting boundary-sensitive categories like "Lowveg" and "Building", yielding maps with clearer contours (Fig. 4). This demonstrates the new network's efficacy in refining object boundaries and capturing fine details, producing segmentation maps with clearer contours and fewer errors.
On ISPRS Vaihingen (83.84% mIoU), it demonstrated particularly strong performance in recognizing small objects such as "Cars" (Fig. 5). This capability is attributed to the PMC module, which adapts to local structures through dynamic receptive field adjustment, resulting in more complete segmentation of small and irregular targets.
On the more diverse LoveDA dataset (53.21% mIoU), it showed significant improvements in structured categories like "Building" and "Road" (Fig. 6). These results highlight the effective collaboration between global context and local detail paths, achieving strong results in complex land segmentation.
 
TABLE IV
ABLATION STUDY OF KEY COMPONENTS
Config	SS2D-Attn	Dual-Path	APFG	MSK	mIoU	FPS
A (Baseline)					52.84	53.86
B					47.30	69.37
C					51.16	46.35
C1					52.44	38.71
D (Ours)					53.21	60.62
E (w/o ECA) 					47.72	62.38
F (w/o PMC)					46.08	68.69

TABLE V
MODEL COMPLEXITY COMPARISON
Model	FLOPs(G)	FPS(fps)	Param.(M)	mIoU(%)
DeepLabV3+[15]
95.81	53.73	42.86	47.65
BANet [13]
52.65	11.51	12.74	49.62
Segmenter [12]
26.84	14.78	48.28	47.18
ABCNet [11]
123.43	29.36	13.42	49.80
UNetFormer[14]
46.94	115.33	11.75	49.72
CM-UNet [4]
48.04	53.86	12.89	52.84
DP-UNet	44.26	60.62	11.30	53.21

C. Further Analysis
Comprehensive experiments conducted on the LoveDA dataset using 1024×1024 images evaluate both the effectiveness and efficiency of our method. 
As shown in Table IV, an ablation study evaluated the effectiveness of each proposed component of DP-UNet. First, replacing the MSAA module with the proposed MSK led to a 56.6% increase in FPS and a 0.77-point improvement in mIoU, demonstrating that spatial attention is more efficient for multi-scale feature fusion when a dedicated detail path is present. Second, removing the internal attention from SS2D (Configuration B) significantly boosted FPS but caused a large performance drop (-5.54 mIoU). Introducing the Detail-Branch (Configuration C1) almost fully recovered this loss, showing its ability to compensate for the reduction in representational capacity. Furthermore, the comparison between additive fusion (Configuration C) and APFG-based fusion (Configuration C1) revealed that adaptive fusion is more effective than simple feature addition. Lastly, removing either the ECA module (Configuration E) or the PMC module (Configuration F) resulted in a significant performance decline, underscores that the high performance of our model is intrinsically linked to the synergistic operation of both components.
As shown in Table V, DP-UNet achieves a better balance between computational efficiency and performance. Compared to the baseline, our model attains higher accuracy (53.21% vs. 52.84% mIoU) while consistently improving efficiency: FLOPs are reduced by 7.9%, parameters by 12.3%, and inference speed (FPS) increased by 12.5%. Moreover, against other efficient models like UNetFormer, DP-UNet improves mIoU by 3.49% with only a slight rise in FLOPs, underscoring its strong cost-effectiveness.
IV. Conclusion
This study successfully transitions from "attention stacking" to fundamental "architecture rethinking." By proposing a dual-path decoupled architecture that explicitly separates global and local processing, we effectively overcome the computational redundancy and performance bottlenecks of the baseline model. This strategic decoupling, coupled with a streamlined attention mechanism, not only significantly enhances inference efficiency but also achieves superior segmentation accuracy on multiple remote sensing benchmarks, delivering a more powerful and practical solution for remote sensing image segmentation.
While effective, the proposed approach has limitations. The current experiments focus on optical remote sensing images with a limited number of land-cover categories (6-7 classes); the performance on multi-spectral or SAR imagery with more complex class distributions remains to be validated. Future work will extend the evaluation to a broader range of sensor types and application scenarios.
