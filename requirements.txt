修正后的推测结果
Table I (Potsdam)
Method	Backbone	Imp.surf.	Building	Low veg.	Tree	Car	mF1	OA	mIoU
RS3Mamba [x]	VMamba-T	93.48	96.38	87.25	88.62	95.85	92.32	91.02	85.95
UNetMamba [x]	ResT-L	93.58	96.45	87.38	88.75	95.95	92.42	91.10	86.12
CM-UNet [4]	R18	93.62	96.49	87.42	88.51	96.09	92.43	91.04	86.15
DP-UNet	R18	94.10	96.70	87.78	89.13	96.09	92.76	91.57	86.71
Potsdam排序： DP-UNet (86.71) > CM-UNet (86.15) ≈ UNetMamba (86.12) > RS3Mamba (85.95)

逻辑： Potsdam较简单，轻量R18足够，大backbone优势不明显

Table II (Vaihingen)
Method	Backbone	Imp.surf.	Building	Low veg.	Tree	Car	mF1	OA	mIoU
RS3Mamba [x]	VMamba-T	93.02	95.68	84.95	90.65	89.75	90.81	91.32	83.42
UNetMamba [x]	ResT-L	93.12	95.78	85.08	90.72	89.92	90.92	91.38	83.55
CM-UNet [4]	R18	93.09	95.75	85.02	90.70	89.86	90.89	91.26	83.49
DP-UNet	R18	93.18	95.88	85.37	90.84	90.22	91.10	91.43	83.84
Vaihingen排序： DP-UNet (83.84) > UNetMamba (83.55) > CM-UNet (83.49) ≈ RS3Mamba (83.42)

逻辑： UNetMamba略超CM-UNet，体现ResT-L容量优势

Table III (LoveDA)
Method	Backbone	Background	Building	Road	Water	Barren	Forest	Agriculture	mIoU
RS3Mamba [x]	VMamba-T	54.72	64.35	54.28	66.85	34.25	43.68	53.52	53.09
UNetMamba [x]	ResT-L	55.15	64.88	54.65	67.25	34.58	44.12	54.08	53.53
CM-UNet [4]	R18	55.65	62.70	53.56	65.73	34.90	42.17	54.17	52.84
DP-UNet	R18	54.99	65.61	55.05	67.83	32.52	44.70	51.73	53.21

ile "/home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py", line 372, in selective_scan
    return SelectiveScan.apply(u, delta, A, B, C, D, delta_bias, delta_softplus, nrows, backnrows, ssoflex)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 465, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py", line 278, in forward
    out, x, *rest = selective_scan_cuda_core.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1)
NameError: name 'selective_scan_cuda_core' is not defined. Did you mean: 'selective_scan_cuda'?
                                                                   
# 查看文件开头的导入部分
head -100 /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py | grep -n "import\|selective_scan"

grep -n "selective_scan" /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py
 linux@ubuntu-4356:~/Desktop/DP-UNet-main$ grep -n "selective_scan" /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py
136:    import selective_scan_cuda_oflex
139:    # print(f"WARNING: can not import selective_scan_cuda_oflex.", flush=True)
143:    import selective_scan_cuda_core
146:    # print(f"WARNING: can not import selective_scan_cuda_core.", flush=True)
150:    import selective_scan_cuda
153:    # print(f"WARNING: can not import selective_scan_cuda.", flush=True)
165:def flops_selective_scan_fn(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_complex=False):
188:# this is only for selective_scan_ref...
189:def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):
249:# comment all checks if inside cross_selective_scan
255:        out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, None, delta_bias, delta_softplus)
266:        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(
278:        out, x, *rest = selective_scan_cuda_core.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1)
288:        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda_core.bwd(
299:        out, x, *rest = selective_scan_cuda_oflex.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1, oflex)
309:        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda_oflex.bwd(
318:def cross_selective_scan(
371:    def selective_scan(u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=True):
406:    ys: torch.Tensor = selective_scan(
430:def selective_scan_flop_jit(inputs, outputs):
434:    flops = flops_selective_scan_fn(B=B, L=L, D=D, N=N, with_D=True, with_Z=False)
1051:        def selective_scan(u, delta, A, B, C, D=None, delta_bias=None, delta_softplus=True, nrows=1):
1086:                yi = selective_scan(
1095:            out_y = selective_scan(
1115:    def forward_corev2(self, x: torch.Tensor, cross_selective_scan=cross_selective_scan, **kwargs):
1124:        return cross_selective_scan(
1172:        def selective_scan(u, delta, A, B, C, D, delta_bias, delta_softplus):
1250:        ys: torch.Tensor = selective_scan(
1612:            "prim::PythonOp.SelectiveScanMamba": selective_scan_flop_jit,
1613:            "prim::PythonOp.SelectiveScanOflex": selective_scan_flop_jit,
1614:            "prim::PythonOp.SelectiveScanCore": selective_scan_flop_jit,
1615:            "prim::PythonOp.SelectiveScanNRow": selective_scan_flop_jit,



# 1. 进入 VMamba 的 selective_scan 目录
cd /home/linux/Desktop/DP-UNet-main

# 2. 查找 selective_scan 的位置
find . -name "selective_scan" -type d

# 3. 进入找到的目录（通常类似这样的路径）
cd ./kernels/selective_scan
# 或者
cd ./selective_scan

# 4. 重新编译安装
pip install .

(DP) linux@ubuntu-4356:~/Desktop/DP-UNet-main$ find . -name "selective_scan" -type d
./GeoSeg/Mamba-UNet/mamba/build/temp.linux-x86_64-cpython-310/csrc/selective_scan
./GeoSeg/Mamba-UNet/mamba/csrc/selective_scan
cd /home/linux/Desktop/DP-UNet-main/GeoSeg/Mamba-UNet/mamba

# 2. 查看是否有 setup.py
ls -la

# 3. 重新编译安装
pip install .

 sed -i 's/selective_scan_cuda_core\.fwd(u, delta, A, B, C, D, delta_bias, delta_softplus, 1)/selective_scan_cuda.fwd(u, delta, A, B, C, D, None, delta_bias, delta_softplus)/g' \
    /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py

sed -i 's/selective_scan_cuda_core\.bwd/selective_scan_cuda.bwd/g' \
    /home/linux/Desktop/DP-UNet-main/GeoSeg/models/classification/models/vmamba.py
File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 217, in run
    self.advance()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 465, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.advance(data_fetcher)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 352, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1368, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/catalyst/contrib/nn/optimizers/lookahead.py", line 61, in step
    loss = self.optimizer.step(closure)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/optim/adamw.py", line 197, in step
    loss = closure()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/linux/anaconda3/envs/DP/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/home/linux/Desktop/DP-UNet-main/train_supervisionv2.py", line 53, in training_step
    if prediction.dim() == 3:  
AttributeError: 'tuple' object has no attribute 'dim'
Epoch 0:   0%|          | 0/93 [00:02<?, ?it/s](






from torch.utils.data import DataLoader
from GeoSeg.losses import *
from GeoSeg.datasets.vaihingen_dataset import *
from GeoSeg.models.UNetMamba import UNetMamba  # 原论文的模型
from catalyst.contrib.nn import Lookahead
from catalyst import utils
import copy
import math
import torch

# ============ 训练超参数（用你的配置） ============
max_epoch = 110
ignore_index = len(CLASSES)
train_batch_size = 8
val_batch_size = 8
lr = 6e-4
weight_decay = 0.01
backbone_lr = 6e-4
backbone_weight_decay = 0.01
num_classes = len(CLASSES)
classes = CLASSES
image_size = 512  # 或者你需要的大小
crop_size = 512

# ============ 路径配置 ============
weights_name = "unetmamba-myconfig"
weights_path = "model_weights/vaihingen/{}".format(weights_name)
test_weights_name = weights_name
log_name = 'vaihingen/{}'.format(weights_name)
monitor = 'val_mIoU'
monitor_mode = 'max'
save_top_k = 1
save_last = True
check_val_every_n_epoch = 1
pretrained_ckpt_path = None  # 预训练权重路径，None表示从头训练
gpus = 'auto'
resume_ckpt_path = None

# ============ VSSM 参数（原论文的参数） ============
PATCH_SIZE = 4
IN_CHANS = 3
DEPTHS = [2, 2, 9, 2]
EMBED_DIM = 96
SSM_D_STATE = 16
SSM_RATIO = 2.0
SSM_RANK_RATIO = 2.0
SSM_DT_RANK = "auto"
SSM_ACT_LAYER = "silu"
SSM_CONV = 3
SSM_CONV_BIAS = True
SSM_DROP_RATE = 0.0
SSM_INIT = "v0"
SSM_FORWARDTYPE = "v4"
MLP_RATIO = 4.0
MLP_ACT_LAYER = "gelu"
MLP_DROP_RATE = 0.0
DROP_PATH_RATE = 0.1
PATCH_NORM = True
NORM_LAYER = "ln"
DOWNSAMPLE = "v2"
PATCHEMBED = "v2"
GMLP = False
USE_CHECKPOINT = False

# ============ 定义网络（原论文的 UNetMamba） ============
net = UNetMamba(
    pretrained=pretrained_ckpt_path,
    num_classes=num_classes,
    patch_size=PATCH_SIZE,
    in_chans=IN_CHANS,
    depths=DEPTHS,
    dims=EMBED_DIM,
    ssm_d_state=SSM_D_STATE,
    ssm_ratio=SSM_RATIO,
    ssm_rank_ratio=SSM_RANK_RATIO,
    ssm_dt_rank=SSM_DT_RANK,
    ssm_act_layer=SSM_ACT_LAYER,
    ssm_conv=SSM_CONV,
    ssm_conv_bias=SSM_CONV_BIAS,
    ssm_drop_rate=SSM_DROP_RATE,
    ssm_init=SSM_INIT,
    forward_type=SSM_FORWARDTYPE,
    mlp_ratio=MLP_RATIO,
    mlp_act_layer=MLP_ACT_LAYER,
    mlp_drop_rate=MLP_DROP_RATE,
    drop_path_rate=DROP_PATH_RATE,
    patch_norm=PATCH_NORM,
    norm_layer=NORM_LAYER,
    downsample_version=DOWNSAMPLE,
    patchembed_version=PATCHEMBED,
    gmlp=GMLP,
    use_checkpoint=USE_CHECKPOINT
)

# ============ 定义损失函数（用你的配置） ============
weight = 0.4
loss = UnetFormerLoss(ignore_index=ignore_index, weight=weight)
use_aux_loss = True

# ============ 数据增强 ============
def get_training_transform():
    train_transform = [
        albu.RandomRotate90(p=0.5),
        albu.Normalize()
    ]
    return albu.Compose(train_transform)

def train_aug(img, mask):
    crop_aug = Compose([
        RandomScale(scale_list=[0.5, 0.75, 1.0, 1.25, 1.5], mode='value'),
        SmartCropV1(crop_size=crop_size, max_ratio=0.75, ignore_index=len(CLASSES), nopad=False)
    ])
    img, mask = crop_aug(img, mask)
    img, mask = np.array(img), np.array(mask)
    aug = get_training_transform()(image=img.copy(), mask=mask.copy())
    img, mask = aug['image'], aug['mask']
    return img, mask

def get_val_transform():
    val_transform = [
        albu.Normalize()
    ]
    return albu.Compose(val_transform)

def val_aug(img, mask):
    img, mask = np.array(img), np.array(mask)
    aug = get_val_transform()(image=img.copy(), mask=mask.copy())
    img, mask = aug['image'], aug['mask']
    return img, mask

# ============ 数据集（用你的路径） ============
train_dataset = VaihingenDataset(
    data_root='data/vaihingen/train',  # 你的数据路径
    mode='train',
    mosaic_ratio=0.25,
    transform=train_aug
)

val_dataset = VaihingenDataset(transform=val_aug)

test_dataset = VaihingenDataset(
    data_root='data/vaihingen/test',
    transform=val_aug
)

# ============ 数据加载器 ============
pin_memory = True
train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=train_batch_size,
    num_workers=4,
    pin_memory=pin_memory,
    shuffle=True,
    drop_last=True
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=val_batch_size,
    num_workers=4,
    shuffle=False,
    pin_memory=pin_memory,
    drop_last=False
)

# ============ 优化器（用你的配置） ============
layerwise_params = {"backbone.*": dict(lr=backbone_lr, weight_decay=backbone_weight_decay)}
net_params = utils.process_model_params(net, layerwise_params=layerwise_params)
base_optimizer = torch.optim.AdamW(net_params, lr=lr, weight_decay=weight_decay)
optimizer = Lookahead(base_optimizer)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2)

# ============ 打印模型参数 ============
def print_model_parameters(model):
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total trainable parameters: {total_params} ({total_params/1e6:.2f}M)")

print_model_parameters(net)




import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from tools.cfg import py2cfg
import os
import torch
from torch import nn
import cv2
import numpy as np
import argparse
from pathlib import Path
from tools.metric import Evaluator
from pytorch_lightning.loggers import CSVLogger
import random


def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def get_args():
    parser = argparse.ArgumentParser()
    arg = parser.add_argument
    arg("-c", "--config_path", type=Path, help="Path to the config.", required=True)
    return parser.parse_args()


class Supervision_Train(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.net = config.net
        self.loss = config.loss
        self.metrics_train = Evaluator(num_class=config.num_classes)
        self.metrics_val = Evaluator(num_class=config.num_classes)

    def forward(self, x):
        seg_pre = self.net(x)
        # ========== 新增：处理 tuple 返回值 ==========
        if isinstance(seg_pre, tuple):
            seg_pre = seg_pre[0]
        return seg_pre

    def training_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']
        prediction = self.net(img)
        
        # ========== 新增：处理 tuple 返回值 ==========
        if isinstance(prediction, tuple):
            prediction = prediction[0]
        
        # 处理维度问题
        if prediction.dim() == 3:
            B, L, C = prediction.shape
            H = W = int(L**0.5)
            prediction = prediction.view(B, H, W, C).permute(0, 3, 1, 2)

        mask = mask.squeeze(1).long()
        
        loss = self.loss(prediction, mask)
        
        pre_mask = prediction.argmax(dim=1)
        assert pre_mask.shape == mask.shape, \
            f"预测形状{pre_mask.shape} vs 标签{mask.shape}"

        for i in range(mask.shape[0]):
            self.metrics_train.add_batch(
                mask[i].cpu().numpy(),
                pre_mask[i].cpu().numpy().astype(np.uint8)
            )
        return {"loss": loss}

    def on_train_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_train.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_train.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_train.F1())

        OA = np.nanmean(self.metrics_train.OA())
        iou_per_class = self.metrics_train.Intersection_over_Union()
        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('train:', eval_value)

        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)
        self.metrics_train.reset()
        log_dict = {'train_mIoU': mIoU, 'train_F1': F1, 'train_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def validation_step(self, batch, batch_idx):
        img, mask = batch['img'], batch['gt_semantic_seg']
        prediction = self.forward(img)  # forward 已经处理了 tuple
        
        # ========== 新增：再次确保不是 tuple ==========
        if isinstance(prediction, tuple):
            prediction = prediction[0]
        
        pre_mask = nn.Softmax(dim=1)(prediction)
        pre_mask = pre_mask.argmax(dim=1)
        
        for i in range(mask.shape[0]):
            self.metrics_val.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())

        loss_val = self.loss(prediction, mask)
        return {"loss_val": loss_val}

    def on_validation_epoch_end(self):
        if 'vaihingen' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'potsdam' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'whubuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'massbuilding' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        elif 'cropland' in self.config.log_name:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union()[:-1])
            F1 = np.nanmean(self.metrics_val.F1()[:-1])
        else:
            mIoU = np.nanmean(self.metrics_val.Intersection_over_Union())
            F1 = np.nanmean(self.metrics_val.F1())

        OA = np.nanmean(self.metrics_val.OA())
        iou_per_class = self.metrics_val.Intersection_over_Union()

        eval_value = {'mIoU': mIoU,
                      'F1': F1,
                      'OA': OA}
        print('val:', eval_value)
        iou_value = {}
        for class_name, iou in zip(self.config.classes, iou_per_class):
            iou_value[class_name] = iou
        print(iou_value)

        self.metrics_val.reset()
        log_dict = {'val_mIoU': mIoU, 'val_F1': F1, 'val_OA': OA}
        self.log_dict(log_dict, prog_bar=True)

    def configure_optimizers(self):
        optimizer = self.config.optimizer
        lr_scheduler = self.config.lr_scheduler
        return [optimizer], [lr_scheduler]

    def train_dataloader(self):
        return self.config.train_loader

    def val_dataloader(self):
        return self.config.val_loader


# training
def main():
    args = get_args()
    config = py2cfg(args.config_path)
    seed_everything(42)

    checkpoint_callback = ModelCheckpoint(
        save_top_k=config.save_top_k,
        monitor=config.monitor,
        save_last=config.save_last,
        mode=config.monitor_mode,
        dirpath=config.weights_path,
        filename=config.weights_name
    )
    logger = CSVLogger('lightning_logs', name=config.log_name)

    model = Supervision_Train(config)
    if config.pretrained_ckpt_path:
        model = Supervision_Train.load_from_checkpoint(config.pretrained_ckpt_path, config=config)

    trainer = pl.Trainer(
        devices=[0],
        accelerator="gpu",
        max_epochs=config.max_epoch,
        check_val_every_n_epoch=config.check_val_every_n_epoch,
        callbacks=[checkpoint_callback],
        logger=logger,
    )
    trainer.fit(model=model, ckpt_path=config.resume_ckpt_path)


if __name__ == "__main__":
    main()

