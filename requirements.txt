From Attention Stacking to Architecture Rethinking: Dual-Path Decoupling for Efficient Remote Sensing Image Segmentation

Zhaoyao Zhang, Jie Sun, Zhuo Yang 


[(Corresponding author: Zhaoyao Zhang)
Zhaoyao Zhang, Jie Sun, and Zhuo Yang are with the School of Data Sciences, Zhejiang University of Finance and Economics, 18 Xuezheng Street, Hangzhou, Zhejiang, 310018 China (e-mail: 2692651809@qq.com; jiesun@zufe.edu.cn; yangzhuomath@163.com).]
Abstract—Remote sensing image segmentation has long struggled with balancing efficiency and accuracy. Most current research focuses on hybrid architectures that combine state-space models with CNNs, with CM-UNet being a notable example. However, its single-path decoding mechanism creates a performance bottleneck, and its multi-level homogeneous attention mechanism introduces significant computational redundancy. This paper proposes a novel Dual-Path U-Net (DP-UNet), using CM-UNet as the baseline. Our key insight is to explicitly decouple the processing of global context and local details, which are inherently distinct tasks. First, we innovate by designing a Decoupled Visual State Space (DVSS) block: one path models long-range dependencies through a simplified state-space model, while the other path integrates Efficient Channel Attention (ECA) and a new Parameter-Modulated Convolution (PMC) module to enhance detail extraction. The features from both paths are fused through an Adaptive Path Fusion Gate (APFG) module. Second, we streamline attention by removing redundant units from the SS2D module in CM-UNet and simplifying its multi-scale fusion module into a Multi-scale Spatial Kernel (MSK) module, which refines its structure to focus solely on spatial attention, reducing model complexity. Experimental results on three benchmarks show that DP-UNet outperforms existing state-of-the-art methods in both accuracy and efficiency. This study demonstrates the potential of explicit decoupling architectures in visual tasks. The codes are available at https://github.com/sharkzzzy/DP-UNet.

Index Terms—Remote Sensing, Semantic Segmentation, Visual State space model

I. Introduction
Remote sensing image segmentation is widely applied in urban planning, environmental monitoring, and disaster assessment. Due to high resolution, multi-scale objects, and complex backgrounds, capturing long-range dependencies while maintaining efficiency remains a key challenge.
To address these challenges, recent research combines lightweight state-space models with CNN-based architectures. Traditional U-Net [1] architectures, whether based on CNNs [2] or Transformers [3], struggle with large-scale scenes and suffer from high computational complexity. Combining state-space models with CNNs leverages both efficiency and the rich features from pretrained CNN models.A notable advancement is CM-UNet [4], which combines a CNN-based encoder with a Mamba-based [5] decoder, employing a CSMamba block with channel-spatial attention gating and a Multi-Scale Attention Aggregation (MSAA) module for feature fusion.
Despite its strong performance across various tasks, CM-UNet still exhibits certain limitations. First, its coupled architecture presents a bottleneck. The CSMamba module processes both global context modeling and local detail refinement within a single sequential pathway. We argue that these are two fundamentally distinct tasks requiring specialized processing. Forcing them to share parameters and computational flow limits the model's capacity to excel at either task optimally. Second, it suffers from attention redundancy. The MSAA module at the feature fusion stage already incorporates both channel and spatial attention. The subsequent reapplication of the same channel and spatial attention mechanisms within the SS2D module. Although this multi-level homogeneous attention design enhances feature focusing to some extent, it inevitably increases model complexity and computational overhead.
To address these issues, we propose DP-UNet, a novel architecture designed for enhanced computational efficiency and segmentation performance. Our approach is guided by two core principles: decoupling and streamlining. First, we introduce a DVSS block to replace the single-branch design, decoupling processing into parallel global context and local detail paths, later fused by an APFG module. Second, we streamline attention by removing redundancies within SS2D and simplifying the multi-scale fusion into a lightweight MSK module, which focuses solely on spatial attention. The contributions of this work are summarized as follows:
We identify the limitation of the coupled single-branch design in the baseline. The proposed DVSS explicitly decouples global context modeling from local detail enhancement, allowing each sub-task to be optimized by a dedicated pathway, thereby unleashing the model’s full potential.
We identify and eliminate dual attention redundancies in the original model. This streamlining strategy involves two key steps: removing the internal spatial and channel attention gating within the SS2D module, and modifying the feature fusion module into the lightweight MSK. These refinements substantially improve model efficiency without sacrificing performance.
Extensive experiments on three widely used public RS datasets—ISPRS Potsdam, Vaihingen, and LoveDA—demonstrate the superiority of the proposed DP-UNet. 


Fig. 1. (a) Overall architecture of DP-UNet. The red arrows indicating upsampling and blue arrows indicating downsampling. (b) The baseline CSMamba block. (c) Our proposed DVSS block.

II. Methodology
The proposed DP-UNet is an improved architecture over CM-UNet, designed to enhance segmentation accuracy and optimize computational efficiency through structural innovations. It achieves these goals by replacing the original MSAA with the MSK module to eliminate redundant computations and by introducing the DVSS block in place of the single-path CSMamba block, thereby decoupling global context modeling from local detail refinement. As illustrated in Fig. 1(a), the network consists of three main components: the ResNet-18-based [6] encoders for extracting multi-level features, the MSK modules for multi-scale feature fusion, and the decoders with DVSS block that utilize a dual-path approach for better semantic understanding.
Specifically, the encoder produces multi-level features F1, F2, F3, F4 from the input image, and MSK fuses F1,F2,F3 (see Section II-B). Each DVSS block processes its input X through: (1) shared feature extraction Fs=SS2DLNX; (2) global path Fg=Fs; (3) local path Fl=ConvECAFs+Fs;Weff, where Weff from Eq.(1) is applied to ECA-recalibrated features; (4) adaptive fusion Y=X+DropPathAPFGFg+APFGFl. The model is trained with L=Lmain+i=13λi⋅Lauxi, where all terms are cross-entropy losses and λi = 0.4, 0.3, 0.2.
A. Main decoder (DVSS)
To clearly articulate the motivation for our improvements, we begin by revisiting the decoder CSMamba component of 
the baseline model. As illustrated in Fig. 1(b), its central component is the two-dimensional state space mechanism (2D-SSM), which captures global spatial context through a four-directional scanning process, please refer to [5] for more details about it. The SS2D module internally integrates both channel and spatial attention mechanisms as gates to enhance its feature selection capabilities. We posit that global semantic context modeling and local spatial detail refinement are distinct tasks, yet they share the same underlying feature foundation. Processing them sequentially within the single-path architecture of the original decoder may prevent either task from achieving optimal performance. The DVSS adopts a "shared base, separate enhancement" design philosophy. Its core idea is to have global and local processing share the same underlying feature source before undergoing task-specific optimization. 
The structure of the DVSS is shown in Fig. 1(c). The input feature X∈ℝB×H×W×C is first normalized and processed by the SS2D module to produce a shared foundational feature Fs. This feature serves both paths: the global path retains Fs directly, while the local detail path processes it as follows. It undergoes channel recalibration through an Efficient Channel Attention (ECA) [7] module, which models cross-channel dependencies via a lightweight 1D convolution. This recalibration helps the subsequent PMC focus on informative channels. The recalibrated feature is enhanced via a residual connection before being fed into the PMC module.
The PMC module is inspired by Learnable Deformable Convolution (LDC) [8], but operates in the parameter domain rather than the spatial domain. This design choice is motivated by a key observation: standard convolutions exhibit a center-dominance tendency, where center weights consistently receive larger magnitudes after training. This occurs partly because the kernel center always corresponds to a well-aligned sampling position across spatial locations, and is repeatedly reinforced by overlapping receptive fields, leading to more stable gradient signals and larger cumulative updates during optimization. Although a sufficiently expressive network could, in principle, learn non-center-dominant kernels from data, standard training does not explicitly encourage such behavior; consequently, gradient-based optimization tends to converge to center-reliant solutions that minimize training loss but remain suboptimal for fine-detail extraction. By shifting the deformation from the spatial to the parameter domain, PMC eliminates the complex coordinate offset and interpolation computations required in traditional LDC, while introducing a lightweight, learnable inhibitory prior to counteract center-dominance.
The core mechanism works as follows: a dynamic mask, correlated with the convolutional kernel weights, adaptively modulates the convolution weights and suppresses the intensity at the kernel's center. This suppression compels the network to gather information from neighboring positions, enhancing sensitivity to edges and fine textures—precisely the local details this path aims to capture.


  
Fig. 2. (a) The workflow of the PMC module. (b) The structure of the APFG module.

As shown in Fig. 2(a), the weight tensor W, of the standard convolution kernel is first summed across the spatial dimensions to generate a channel strength matrix S, which characterizes the importance of each input-output channel pair. Then, a dynamic mask Md is generated by multiplying the learnable mask matrix Wm, the fixed center mask Mc (with a center value of 1 and zeros elsewhere), and the previously mentioned strength matrix S. A learnable scaling factor θ is then introduced, and the original convolution weights are modulated using the expression in Eq. (1), yielding the effective convolution weights Weff. 
Weff=W⊙B−θ×Md#1
where B is a tensor of all ones, and ⊙ represents element-wise multiplication. These modulated weights Weff are applied to convolve the feature ECAFs+Fs, producing the local path output Fl.
Finally, the outputs of both paths are fused via the APFG module. As shown in Fig. 2(b), APFG applies channel-wise attention to each path independently: it performs global average pooling, followed by a bottleneck structure (reduction and expansion via linear layers) with sigmoid activation to generate attention weights. Each path's feature is recalibrated by its own attention weights, and the results are summed: Y=X+DropPathAPFGFg+APFGFl. We adopt SiLU in SS2D following the original VMamba [5] design for architectural consistency, while using GeLU in APFG for its smoother gradient profile suited to fusing features from heterogeneous paths.

B. Multi-Scale Fusion Module (MSK)
In the feature fusion stage, we reconsider the role of attention mechanisms and propose the more computationally efficient MSK module. This design is motivated by two key observations. First, the ECA module within the DVSS block’s local detail path already accounts for inter-channel dependencies, making additional channel attention at fusion redundant. Second, the core challenge during this stage is bridging the semantic gap and spatial misalignment across multi-resolution   feature   maps,   where   integrating   spatial 

Fig. 3.The structure of the MSK module.

information is far more critical than recalibrating channels. Thus, we discard complex hybrid attention in favor of a spatial-only feature fusion strategy. 
The MSK module integrates multi-scale encoder features through an efficient spatial-focused fusion process. As illustrated in Fig. 3, it takes the channel-wise concatenation of encoder features (F₁, F₂, F₃) as input. A 1×1 convolution first compresses the features to C/4 channels. In this compressed space, three standard convolutions (3×3, 5×5, 7×7) are applied in parallel to extract multi-scale spatial patterns, and their outputs are summed together. Although these are regular convolutions, the overall cost remains low because all operations are performed on the compressed feature map with only C/4 channels. For spatial attention, average and max pooling are applied along the channel axis, and their results are concatenated and processed by a 7×7 convolution with sigmoid activation, producing an attention map that modulates the fused features via multiplication. A final 1×1 convolution restores the channel dimension.
This design improves efficiency compared to the baseline MSAA by operating in a compressed feature space and using only spatial attention. A standard channel attention module requires C'²/2 parameters, while the spatial attention here uses only 98 parameters (a 7×7 conv with 2 input channels). When C'=32, this reduces attention-related parameters from 512 to 98, achieving over 80% reduction.

III. Results
A. Datasets and Implementation Details
We evaluated the proposed DP-UNet on three public remote sensing image segmentation datasets: LoveDA [9], ISPRS Potsdam,  and  ISPRS Vaihingen.  The ISPRS Potsdam dataset

TABLE I
Experimental Results on ISPRS Potsdam Dataset
Method	Backbone	Imp.surf.	Building	Lowveg.	Tree	Car	mF1	OA	mIoU
ABCNet [11]	R18	93.28	95.91	86.78	87.81	95.49	91.85	90.51	85.17
Segmenter [12]	ViT-T	91.56	95.38	85.43	85.01	88.51	89.27	88.78	80.71
BANet [13]	ResT-L	93.36	96.70	87.48	89.11	96.05	92.55	91.01	86.36
UNetFormer[14]	R18	93.69	96.10	87.27	88.72	96.58	92.47	91.07	86.23
CM-UNet [4]	R18	93.62	96.49	87.42	88.51	96.09	92.43	91.04	86.15
DP-UNet	R18	94.10	96.70	87.78	89.13	96.09	92.76	91.57	86.71

			
			
(a)	(b)	(c)	(d)

Fig. 4. Qualitative performance comparisons on the ISPRS Potsdam. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet. 

TABLE II
Experimental Results on ISPRRS Vaihingen Dataset
Method	Backbone	Imp.surf.	Building	Lowveg.	Tree	Car	mF1	OA	mIoU
ABCNet  [11]	R18	92.72	95.20	84.55	89.71	85.35	89.52	90.78	81.31
RS3Mamba	R18-VMamba-T	93.07	95.70	85.06	90.94	88.76	90.67	91.25	83.13
Segmenter  [12]	ViT-T	89.84	93.05	81.23	88.90	67.66	84.10	88.17	73.60
BANet  [13]	ResT-L	92.22	95.23	83.81	89.92	86.82	89.63	90.52	81.45
UNetFormer [14]	R18	93.07	95.42	85.03	90.74	88.99	90.65	91.21	83.09
CM-UNet  [4]	R18	93.09	95.75	85.02	90.70	89.86	90.89	91.26	83.49
DP-UNet	R18	93.18	95.88	85.37	90.84	90.22	91.10	91.43	83.84

			
			
(a)	(b)	(c)	(d)

Fig. 5. Qualitative performance comparisons on the ISPRS Vaihingen. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet. 

consists of 23 images (excluding image 710 due to erroneous 
annotations) for training and 14 for testing. The ISPRS Vaihingen dataset includes 12 training blocks and 4 testing blocks. The LoveDA dataset contains 1,156 training images and 677 test images. The segmentation results were evaluated using several standard metrics: mean F1-score (mF1), mean Intersection over Union (mIoU), and Overall Accuracy (OA).
TABLE III
Experimental Results on LoveDA Dataset
Method	Backbone	Background	Building	Road	Water	Barren	Forest	Agriculture	mIoU
DeepLabV3+[15]	R50	43.02	50.92	52.04	74.43	10.40	44.21	58.59	47.65
Segmenter [12]	ViT-T	38.03	50.74	48.76	77.40	13.36	43.48	58.20	47.18
ABCNet [11]	ViT-R50	53.00	62.18	52.42	62.02	29.80	41.61	47.27	49.80
RS3Mamba	R18-VMamba-T	54.01	57.13	54.62	61.99	30.64	38.12	43.41	48.56
UNetFormer [14]	R18	53.44	56.76	51.48	64.48	34.20	39.51	48.20	49.72
CM-UNet [4]	R18	55.65	62.70	53.56	65.73	34.90	42.17	54.17	52.84
DP-UNet	R18	54.99	65.61	55.05	67.83	32.52	44.70	51.73	53.21

			
			
(a)	(b)	(c)	(d)

Fig. 6. Qualitative performance comparisons on the Loveda. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet.

All experiments were implemented in PyTorch on an NVIDIA RTX 3090 GPU. We used the AdamW optimizer with an initial learning rate of 6e-4 and a cosine annealing schedule. For training, input images were randomly cropped to 512×512 patches and augmented through random scaling (0.5-1.5), flipping, rotation, and brightness/contrast adjustment (limit=0.25, p=0.25). All inputs were normalized. During testing, we used the original 1024×1024 resolution and applied test-time flipping for accuracy.

B. Performance Comparison
To validate its effectiveness, we conducted comparisons with state-of-the-art methods on three public datasets. DP-UNet achieved state-of-the-art performance across all benchmarks, demonstrating strong generalization capabilities. As detailed in Table I-III:
On ISPRS Potsdam (86.71% mIoU), it excelled in segmenting boundary-sensitive categories like "Lowveg" and "Building", yielding maps with clearer contours (Fig. 4). This demonstrates the new network's efficacy in refining object boundaries and capturing fine details, producing segmentation maps with clearer contours and fewer errors.
On ISPRS Vaihingen (83.84% mIoU), it demonstrated particularly strong performance in recognizing small objects such as "Cars" (Fig. 5). This capability is attributed to the PMC module, which adapts to local structures through dynamic receptive field adjustment, resulting in more complete segmentation of small and irregular targets.
On the more diverse LoveDA dataset (53.21% mIoU), it showed significant improvements in structured categories like "Building" and "Road" (Fig. 6). These results highlight the effective collaboration between global context and local detail paths, achieving strong results in complex land segmentation.

TABLE IV
Ablation Study of Key Components
Config	SS2D-Attn	Dual-Path	APFG	MSK	mIoU	FPS
A (Baseline)					52.84	53.86
B					47.30	69.37
C					51.16	46.35
C1					52.44	38.71
D (Ours)					53.21	60.62
E (w/o ECA) 					47.72	62.38
F (w/o PMC)					46.08	68.69

TABLE V
Model Complexity Comparison
Model	FLOPs(G)	FPS(fps)	Param.(M)	mIoU(%)
DeepLabV3+[15]	95.81	53.73	42.86	47.65
Segmenter [12]	26.84	14.78	48.28	47.18
ABCNet [11]	123.43	29.36	13.42	49.80
RS3Mamba	157.88	24.86	43.32	48.56
UNetFormer[14]	46.94	115.33	11.75	49.72
CM-UNet [4]	48.04	53.86	12.89	52.84
DP-UNet	44.26	60.62	11.30	53.21

C. Further Analysis
Comprehensive experiments conducted on the LoveDA dataset using 1024×1024 images evaluate both the effectiveness and efficiency of our method. 
As shown in Table IV, an ablation study evaluated the effectiveness of each proposed component of DP-UNet. First, replacing the MSAA module with the proposed MSK led to a 56.6% increase in FPS and a 0.77-point improvement in mIoU, demonstrating that spatial attention is more efficient for multi-scale feature fusion when a dedicated detail path is present. Second, removing the internal attention from SS2D (Configuration B) significantly boosted FPS but caused a large performance drop (-5.54 mIoU). Introducing the Detail-Branch (Configuration C1) almost fully recovered this loss, showing its ability to compensate for the reduction in representational capacity. Furthermore, the comparison between additive fusion (Configuration C) and APFG-based fusion (Configuration C1) revealed that adaptive fusion is more effective than simple feature addition. Lastly, removing either the ECA module (Configuration E) or the PMC module (Configuration F) resulted in a significant performance decline, underscores that the high performance of our model is intrinsically linked to the synergistic operation of both components.
As shown in Table V, DP-UNet achieves a better balance between computational efficiency and performance. Compared to the baseline, our model attains higher accuracy (53.21% vs. 52.84% mIoU) while consistently improving efficiency: FLOPs are reduced by 7.9%, parameters by 12.3%, and inference speed (FPS) increased by 12.5%. Moreover, against other efficient models like UNetFormer, DP-UNet improves mIoU by 3.49% with only a slight rise in FLOPs, underscoring its strong cost-effectiveness.
IV. Conclusion
This study successfully transitions from "attention stacking" to fundamental "architecture rethinking." By proposing a dual-path decoupled architecture that explicitly separates global and local processing, we effectively overcome the computational redundancy and performance bottlenecks of the baseline model. This strategic decoupling, coupled with a streamlined attention mechanism, not only significantly enhances inference efficiency but also achieves superior segmentation accuracy on multiple remote sensing benchmarks, delivering a more powerful and practical solution for remote sensing image segmentation.
While effective, the proposed approach has limitations. The current experiments focus on optical remote sensing images with a limited number of land-cover categories (6-7 classes); the performance on multi-spectral or SAR imagery with more complex class distributions remains to be validated. Future work will extend the evaluation to a broader range of sensor types and application scenarios.

References
[1]	O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Networks for Biomedical Image Segmentation,” vol. 9351, pp. 234–241, 2015, doi: 10.1007/978-3-319-24574-4_28.
[2]	“ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 162, pp. 94–114, Apr. 2020, doi: 10.1016/j.isprsjprs.2020.01.013.
[3]	X. He, Y. Zhou, J. Zhao, D. Zhang, R. Yao, and Y. Xue, “Swin Transformer Embedding UNet for Remote Sensing Image Semantic Segmentation,” IEEE Trans. Geosci. Remote Sensing, vol. 60, pp. 1–15, 2022, doi: 10.1109/TGRS.2022.3144165.
[4]	M. Liu, J. Dan, Z. Lu, Y. Yu, Y. Li, and X. Li, “CM-UNet: Hybrid CNN-Mamba UNet for Remote Sensing Image Semantic Segmentation,” May 17, 2024, arXiv: arXiv:2405.10530. doi: 10.48550/arXiv.2405.10530.
[5]	Y. Liu et al., “VMamba: Visual State Space Model,” Dec. 29, 2024, arXiv: arXiv:2401.10166. doi: 10.48550/arXiv.2401.10166.
[6]	K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” Dec. 10, 2015, arXiv: arXiv:1512.03385. doi: 10.48550/arXiv.1512.03385.
[7]	Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks,” Apr. 07, 2020, arXiv: arXiv:1910.03151. doi: 10.48550/arXiv.1910.03151.
[8]	X. Zhang et al., “LDConv: Linear deformable convolution for improving convolutional neural networks,” Nov. 01, 2023, arXiv. doi: 10.48550/arXiv.2311.11587.
[9]	J. Wang, Z. Zheng, A. Ma, X. Lu, and Y. Zhong, “LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation,” May 31, 2022, arXiv: arXiv:2110.08733. doi: 10.48550/arXiv.2110.08733.
[10]	J. Fu et al., “Dual Attention Network for Scene Segmentation,” Apr. 21, 2019, arXiv: arXiv:1809.02983. doi: 10.48550/arXiv.1809.02983.
[11]	R. Li and C. Duan, “ABCNet: Attentive Bilateral Contextual Network for Efficient Semantic Segmentation of Fine-Resolution Remote Sensing Images,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 181, pp. 84–98, Nov. 2021, doi: 10.1016/j.isprsjprs.2021.09.005.
[12]	R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Transformer for Semantic Segmentation,” Sept. 02, 2021, arXiv: arXiv:2105.05633. doi: 10.48550/arXiv.2105.05633.
[13]	Q. Zhou, Y. Qiang, Y. Mo, X. Wu, and L. J. Latecki, “BANet: Boundary-Assistant Encoder-Decoder Network for Semantic Segmentation,” IEEE Trans. Intell. Transport. Syst., vol. 23, no. 12, pp. 25259–25270, Dec. 2022, doi: 10.1109/TITS.2022.3194213.
[14]	L. Wang et al., “UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 190, pp. 196–214, 2022, doi: 10.1016/j.isprsjprs.2022.06.008.
[15]	L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” Aug. 22, 2018, arXiv: arXiv:1802.02611. doi: 10.48550/arXiv.1802.02611.
