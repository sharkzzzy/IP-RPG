The MSAA module at the feature fusion stage already incorporates both channel and spatial attention, yet similar channel/spatial attention gating is reapplied inside the SS2D module, leading to redundant attention operations.

(I \in \mathbb{R}^{H \times W \times 3})
