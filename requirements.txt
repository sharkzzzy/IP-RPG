I. Introduction
Remote sensing image segmentation is widely applied in urban planning, environmental monitoring, and disaster assessment. Due to high resolution, multi-scale objects, and complex backgrounds, capturing long-range dependencies while maintaining efficiency remains a key challenge.
To address these challenges, recent research combines lightweight state-space models with CNNs. Traditional U-Net [1] architectures, whether CNN-based or Transformer-based, struggle with large-scale scenes and high complexity. A notable advancement is CM-UNet [2], which combines a CNN-based encoder with a Mamba-based decoder, employing 
a CSMamba block with channel-spatial attention gating and a Multi-Scale Attention Aggregation (MSAA) module for feature fusion.
Despite its strong performance across various tasks, CM-UNet still exhibits certain limitations. First, its coupled architecture presents a bottleneck. The CSMamba module processes both global context modeling and local detail refinement within a single sequential pathway. We argue that these are two fundamentally distinct tasks requiring specialized processing. Forcing them to share parameters and computational flow limits the model's capacity to excel at either task optimally. Second, it suffers from attention redundancy. The MSAA module at the feature fusion stage already incorporates both channel and spatial attention, yet similar channel/spatial attention gating is reapplied inside the SS2D module, leading to redundant attention operations. Although this multi-level homogeneous attention design enhances feature focusing to some extent, it inevitably increases model complexity and computational overhead.
To address these issues, we propose DP-UNet, a novel architecture designed for enhanced computational efficiency and segmentation performance. Our approach is guided by two core principles: decoupling and streamlining. First, we introduce a DVSS block to replace the single-branch design, decoupling processing into parallel global context and local detail paths, later fused by an APFG module. Second, we streamline attention by removing redundancies within SS2D and simplifying the multi-scale fusion into a lightweight MSK module, which focuses solely on spatial attention. The contributions of this work are summarized as follows:
	We identify the limitation of the coupled single-branch design in the baseline. The proposed DVSS explicitly decouples global context modeling from local detail enhancement, allowing each sub-task to be optimized by a dedicated pathway, thereby unleashing the model’s full potential.
	We identify and eliminate dual attention redundancies in the original model. This streamlining strategy involves two key steps: removing the internal spatial and channel attention gating within the SS2D module, and modifying the feature fusion module into the lightweight MSK. These refinements substantially improve model efficiency without sacrificing performance.
	Extensive experiments on three widely used public RS datasets—ISPRS Potsdam, Vaihingen, and LoveDA—demonstrate the superiority of the proposed DP-UNet. 
 
 
	 (a) Overall architecture of DP-UNet. The red arrows indicating upsampling and blue arrows indicating downsampling. (b) The baseline CSMamba block. (c) Our proposed DVSS block. 

 
II. Methodology
The proposed DP-UNet is an improved architecture over CM-UNet, designed to enhance segmentation accuracy and optimize computational efficiency through structural innovations. It achieves these goals by replacing the original MSAA with the MSK module to eliminate redundant computations and by introducing the DVSS block in place of the single-path CSMamba block, thereby decoupling global context modeling from local detail refinement. As illustrated in Fig. 1(a), the network consists of three main components: a ResNet-18-based [3] encoder for multi-level feature extraction, MSK modules for multi-scale feature fusion, and a DVSS-based decoder for progressive upsampling and refinement.
Given an input image I\in R^{H\times W\times\mathbb{3}}, the encoder produces four-level features \{F_1,F_2,F_3,F_4\} at 1/4, 1/8, 1/16, and 1/32 of the original resolution. Three MSK modules enhance \{F_1,F_2,F_3\} through cross-scale fusion to obtain enriched skip features \{F_1^\prime,F_2^\prime,F_3^\prime\} (Section II-B), while F_4 is preserved. The decoder reconstructs the segmentation map through four blocks: Block 4 processes F_4, and Blocks 3-1 progressively fuse with \{F_3^\prime,F_2^\prime,F_1^\prime\} via skip connections and DVSS blocks (Section II-A). The final prediction P is produced by a 1\ \times1 convolution head and bilinearly upsampled to the original resolution. Formally:
{F1,F2,F3,F4}=EncI, Fi'=MSKiF1,F2,F3
D4=DVSS4F4,Di=DVSSiProjUpDi+1,Fi'(1)
P=HeadD1,L=JP,Y+λi=24JPi,Y
where \mathrm{Enc}\left(\cdot\right) denotes the encoder, \mathrm{MS}\mathrm{K}_\mathrm{i} fuses multi-scale features with the other two scales resized to the target resolution, \mathrm{Proj}\left(\cdot\right) is a point-wise projection for channel reduction, and \mathrm{Up}\left(\cdot\right) denotes bilinear upsampling. P_i are auxiliary predictions from decoder stages 2-4, J\left(\cdot\right) is a joint loss combining cross-entropy and Dice loss, and\ \lambda=0.4 is the weight for auxiliary losses.

A. Main decoder (DVSS)
The decoder consists of four DVSS blocks: Block 4 refines F₄, while Blocks 3, 2, 1 progressively fuse upsampled features with skip features F_3^\prime,F_2^\prime,F_1^\prime via concatenation, 1\times1 projection, and DVSS processing. Each DVSS decouples global and local processing—the local path applies ECA for channel recalibration followed by PMC-modulated convolution\ (W_{eff}), then both paths are fused by APFG. Auxiliary heads at Blocks 2–4 provide deep supervision. 
To motivate our design, we revisit the baseline CSMamba decoder. As shown in Fig. 1(b), its core SS2D module captures global context via four-directional 2D-SSM scanning [4] and employs internal channel/spatial attention gates to enhance its feature selection capabilities. We posit that global semantic context modeling and local spatial detail refinement are distinct tasks, yet they share the same underlying feature foundation. Processing them sequentially within the single-path architecture of the original decoder may prevent either task from achieving optimal performance. The DVSS adopts a shared base, separate enhancement design, where global and local paths share the same feature source before task-specific optimization.
The structure of the DVSS is shown in Fig. 1(c). The input feature X\in\mathbb{R}^{\left(B\times H\times W\times C\right)} is first normalized and processed by the SS2D module to produce a shared foundational feature F_s. This feature serves both paths: the global path retains F_s directly as F_g, while the local detail path processes it as follows. It undergoes channel recalibration through an Efficient Channel Attention (ECA) [5] module, which models cross-channel dependencies via a lightweight 1D convolution. This recalibration helps the subsequent PMC focus on informative channels. The recalibrated feature is enhanced via a residual connection before being fed into the PMC module.
The PMC module is inspired by Learnable Deformable Convolution (LDC) [6], but operates in the parameter domain rather than the spatial domain. This design choice is motivated by a key observation: standard convolutions exhibit a center-dominance tendency, where center weights consistently receive larger magnitudes after training. This occurs partly because the kernel center always corresponds to a well-aligned sampling position across spatial locations, and is repeatedly reinforced by overlapping receptive fields, leading to more stable gradient signals and larger cumulative updates during  
 
	 (a) The workflow of the PMC module. (b) The structure of the APFG module. (c) The structure of the MSK module.
 

 
optimization. Although a sufficiently expressive network could, in principle, learn non-center-dominant kernels from data, standard training does not explicitly encourage such behavior; consequently, gradient-based optimization tends to converge to center-reliant solutions that minimize training loss but remain suboptimal for fine-detail extraction. By shifting the deformation from the spatial to the parameter domain, PMC eliminates the complex coordinate offset and interpolation computations required in traditional LDC, while introducing a lightweight, learnable inhibitory prior to counteract center-dominance.
The core mechanism works as follows: a dynamic mask, correlated with the convolutional kernel weights, adaptively modulates the convolution weights and suppresses the intensity at the kernel's center. This suppression compels the network to gather information from neighboring positions, enhancing sensitivity to edges and fine textures—precisely the local details this path aims to capture.
As shown in Fig. 2(a), the weight tensor W of the standard convolution kernel is first summed across the spatial dimensions to generate a channel strength matrix S, which characterizes the importance of each input-output channel pair. Then, a dynamic mask M_d is generated by multiplying the learnable mask matrix W_m, the fixed center mask M_c (with a center value of 1 and zeros elsewhere), and the previously mentioned strength matrix S. A learnable scaling factor θ is then introduced, and the original convolution weights are modulated using the expression in Eq. (2), yielding the effective convolution weights W_{eff}.
Weff=W⊙B-θ×Md#2
where B is a tensor of all ones, and \odot represents element-wise multiplication. These modulated weights W_{eff} are applied to convolve the feature \left(ECA\left(F_s\right)+F_s\right), producing the local path output F_l.
Finally, the outputs of both paths are fused via the APFG module. As shown in Fig. 2(b), a shared APFG module applies channel-wise attention to each path independently: it performs global average pooling, followed by a bottleneck structure (reduction and expansion via linear layers) with sigmoid activation to generate attention weights. Each path's feature is recalibrated by its own attention weights, and the results are summed: 
Y=X+DropPathAPFGFg+APFGFl#3
We adopt SiLU in SS2D following the original VMamba [4] design for architectural consistency, while using GeLU in APFG for its smoother gradient profile suited to fusing features from heterogeneous paths.

B. Multi-Scale Fusion Module (MSK)
In the feature fusion stage, we reconsider the role of attention mechanisms and propose the more computationally efficient MSK module. This design is motivated by two key observations. First, the ECA module within the DVSS block's local detail path already accounts for inter-channel dependencies, making additional channel attention at fusion redundant. Second, the core challenge during this stage is bridging the semantic gap and spatial misalignment across multi-resolution feature maps, where integrating spatial information is far more critical than recalibrating channels. Thus, we discard complex hybrid attention in favor of a spatial-only feature fusion strategy.
Three MSK modules operate at different encoder scales to generate {F₁',F₂',F₃'}. For each MSK, the other two scales are first resized via bilinear interpolation to match the target resolution, then concatenated with the target feature as input. 
	
EXPERIMENTAL RESULTS ON ISPRS POTSDAM DATASET
Method	Backbone	Imp.surf.	Building	Lowveg.	Tree	Car	mF1	OA	mIoU
ABCNet [9] 
R18	93.28	95.91	86.78	87.81	95.49	91.85	90.51	85.17
Segmenter [10] 
ViT-T	91.56	95.38	85.43	85.01	88.51	89.27	88.78	80.71
BANet [11] 
ResT-L	93.36	96.70	87.48	89.11	96.05	92.55	91.01	86.36
UnetFormer [12] 
R18	93.69	96.10	87.27	88.72	96.58	92.47	91.07	86.23
CM-UNet [2] 
R18	93.62	96.49	87.42	88.51	96.09	92.43	91.04	86.15
DP-UNet	R18	94.10	96.70	87.78	89.13	96.09	92.76	91.57	86.71

 	 	 	 
 	 	 	 
(a)	(b)	(c)	(d)
 
	 Qualitative performance comparisons on the ISPRS Potsdam. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet. 

	
EXPERIMENTAL RESULTS ON ISPRS VAIHINGEN DATASET
Method	Backbone	Imp.surf.	Building	Lowveg.	Tree	Car	mF1	OA	mIoU
ABCNet [9] 
R18	92.72	95.20	84.55	89.71	85.35	89.52	90.78	81.31
RS3Mamba [13] 
R18-VM-T	93.07	95.70	85.06	90.94	88.76	90.67	91.25	83.13
Segmenter [10] 
ViT-T	89.84	93.05	81.23	88.90	67.66	84.10	88.17	73.60
BANet [11] 
ResT-L	92.22	95.23	83.81	89.92	86.82	89.63	90.52	81.45
UNetFormer [12] 
R18	93.07	95.42	85.03	90.74	88.99	90.65	91.21	83.09
CM-UNet [2] 
R18	93.09	95.75	85.02	90.70	89.86	90.89	91.26	83.49
DP-UNet	R18	93.18	95.88	85.37	90.84	90.22	91.10	91.43	83.84

 	 	 	 
 	 	 	 
(a)	(b)	(c)	(d)
 
	 Qualitative performance comparisons on the ISPRS Vaihingen. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet. 

As illustrated in Fig. 2(c), the concatenated features are first compressed by a 1\times1 convolution to C/4 channels. In this compressed space, three standard convolutions (3×3, 5×5, 7×7) are applied in parallel to extract multi-scale spatial patterns, and their outputs are summed together. Although these are regular convolutions, the overall cost remains low because all operations are performed on the compressed feature map with only C/4 channels. For spatial attention, average and max 
	
EXPERIMENTAL RESULTS ON LOVEDA DATASET
Method	Backbone	Background	Building	Road	Water	Barren	Forest	Agriculture	mIoU
Segmenter [10] 
ViT-T	38.03	50.74	48.76	77.40	13.36	43.48	58.20	47.18
ABCNet [9] 
ViT-R50	53.00	62.18	52.42	62.02	29.80	41.61	47.27	49.80
RS3Mamba [13] 
R18-VM-T	54.01	57.13	54.62	61.99	30.64	38.12	43.41	48.56
UNetFormer [12] 
R18	53.44	56.76	51.48	64.48	34.20	39.51	48.20	49.72
CM-UNet [2] 
R18	55.65	62.70	53.56	65.73	34.90	42.17	54.17	52.84
DP-UNet	R18	54.99	65.61	55.05	67.83	32.52	44.70	51.73	53.21

 	 	 	 
 	 	 	 
(a)	(b)	(c)	(d)
 
	 Qualitative performance comparisons on the Loveda. (a) NIRRG images, (b) UNet-Former, (c) CM-UNet, (d) DP-UNet.

pooling are applied along the channel axis, and their results are concatenated and processed by a 7×7 convolution with sigmoid activation, producing an attention map that modulates the fused features via multiplication. A final 1×1 convolution restores the output to the target channel dimension.
This design improves efficiency compared to the baseline MSAA by operating in compressed space (C/4 channels) with spatial-only attention, reducing attention parameters by over 80% (98 vs. 512 when C^\prime=32).

III. Results
A. Datasets and Implementation Details
We evaluated the proposed DP-UNet on three public remote sensing image segmentation datasets: LoveDA [7], ISPRS Potsdam, and ISPRS Vaihingen [8]. ISPRS Potsdam contains 23/14 images for training/testing (excluding erroneous image 710); Vaihingen has 12/4 blocks; LoveDA has 1,156/677 images. The segmentation results were evaluated using several standard metrics: mean F1-score (mF1), mean Intersection over Union (mIoU), and Overall Accuracy (OA). Experiments used PyTorch on an RTX 3090 GPU with AdamW optimizer (lr=6e-4, cosine schedule). Training used 512×512 crops with standard augmentations (scaling, flipping, rotation, color jittering). Testing used 1024×1024 resolution with test-time flipping.

B. Performance Comparison
As shown in TABLE I-III, DP-UNet achieved state-of-the-art performance among the compared methods across all benchmarks. On ISPRS Potsdam (86.71% mIoU), it excelled in boundary-sensitive categories like Lowveg and Building, yielding clearer contours (Fig. 3). On ISPRS Vaihingen (83.84% mIoU), it showed strong performance on small objects like 
	
ABLATION STUDY OF KEY COMPONENTS
Config	Dual-Path	APFG	Fusion	mIoU	FPS
A (Baseline)			MSAA	52.84	53.86
B (A w/o SS2D-Attn)			MSAA	47.30	69.37
C (+Dual-Path)			MSAA	51.16	46.35
C1 (+APFG)			MSAA	52.44	38.71
D (Ours)			MSK	53.21	60.62
E (D w/o ECA)			MSK	47.72	62.38
F (D w/o PMC)			MSK	46.08	68.69

	
MODEL COMPLEXITY COMPARISON
Model	FLOPs(G)	FPS(fps)	Param.(M)	mIoU(%)
Segmenter [10] 
26.84	14.78	48.28	47.18
ABCNet [9] 
123.43	29.36	13.42	49.80
RS3Mamba  [13] 
157.88	24.86	43.32	48.56
UnetFormer [12] 
46.94	115.33	11.75	49.72
CM-UNet [2] 
48.04	53.86	12.89	52.84
DP-UNet	44.26	60.62	11.30	53.21

Cars (Fig. 4), benefiting from the dedicated local-detail enhancement path (including PMC). On LoveDA (53.21% mIoU), it improved structured categories like Building and Road (Fig. 5), highlighting global–local collaboration.

C. Further Analysis
Comprehensive experiments conducted on the LoveDA dataset using 1024×1024 images evaluate both the effectiveness and efficiency of our method. 
As shown in TABLE IV, an ablation study evaluated the effectiveness of each proposed component of DP-UNet. First, replacing the MSAA module with the proposed MSK led to a 56.6% increase in FPS and a 0.77-point improvement in mIoU, demonstrating that spatial attention is more efficient for multi-scale feature fusion when a dedicated detail path is present. Second, removing the internal attention from SS2D (Configuration B) significantly boosted FPS but caused a large performance drop (-5.54 mIoU). Introducing the Detail-Branch (Configuration C1) almost fully recovered this loss, showing its ability to compensate for the reduction in representational capacity. Furthermore, the comparison between additive fusion (Configuration C) and APFG-based fusion (Configuration C1) revealed that adaptive fusion is more effective than simple feature addition. Lastly, removing either the ECA module (Configuration E) or the PMC module (Configuration F) resulted in a significant performance decline, underscores that the high performance of our model is intrinsically linked to the synergistic operation of both components.
As shown in TABLE V, DP-UNet achieves a better balance between computational efficiency and performance. Compared to the baseline, our model attains higher accuracy (53.21% vs. 52.84% mIoU) while consistently improving efficiency: FLOPs are reduced by 7.9%, parameters by 12.3%, and inference speed (FPS) increased by 12.5%. Moreover, against other efficient models like UNetFormer, DP-UNet improves mIoU by 3.49% with only a slight rise in FLOPs, underscoring its strong cost-effectiveness.
IV. Conclusion
This study transitions from attention stacking to architecture rethinking. By proposing a dual-path decoupled architecture that explicitly separates global and local processing, we overcome the computational redundancy and performance bottlenecks of the baseline. This strategic decoupling, coupled with streamlined attention, enhances both efficiency and accuracy on multiple remote sensing benchmarks.
While effective, the proposed approach has limitations. The dual-path design increases architectural complexity and branch interactions, which may require more careful optimization and regularization on very small datasets, potentially increasing the risk of overfitting. Future work will extend the evaluation to a broader range of sensor types and application scenarios.
