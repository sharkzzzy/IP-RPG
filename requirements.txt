The proposed DP-UNet is an improved architecture over CM-UNet, designed to enhance segmentation accuracy and optimize computational efficiency through structural innovations. It achieves these goals by replacing the original MSAA with the MSK module to eliminate redundant computations and by introducing the DVSS block in place of the single-path CSMamba block, thereby decoupling global context modeling from local detail refinement. As illustrated in Fig. 1(a), the network consists of three main components: a ResNet-18-based [3] encoder for multi-level feature extraction, MSK modules for multi-scale feature fusion, and a DVSS-based decoder for progressive upsampling and refinement.
Given an input image I\in R^{H\times W\times\mathbb{3}}, the encoder produces four-level features \{F_1,F_2,F_3,F_4\} at 1/4, 1/8, 1/16, and 1/32 of the original resolution. Three MSK modules enhance \{F_1,F_2,F_3\} through cross-scale fusion to obtain enriched skip features \{F_1^\prime,F_2^\prime,F_3^\prime\} (Section II-B), while F_4 is preserved. The decoder progressively reconstructs the segmentation map through four DVSS blocks, detailed in Section II-A. The final prediction P is produced by a 1\ \times1 convolution head and bilinearly upsampled to the original resolution. Formally:
{F1,F2,F3,F4}=EncI, Fi'=MSKiF1,F2,F3
D4=DVSS4F4,Di=DVSSiProjUpDi+1,Fi'(1)
P=HeadD1,L=JP,Y+λi=24JPi,Y
where \mathrm{Enc}\left(\cdot\right) denotes the encoder, \mathrm{MS}\mathrm{K}_\mathrm{i} fuses multi-scale features with the other two scales resized to the target resolution, \mathrm{Proj}\left(\cdot\right) is a point-wise projection for channel reduction, and \mathrm{Up}\left(\cdot\right) denotes bilinear upsampling. P_i are auxiliary predictions from DVSS blocks 2-4, J\left(\cdot\right) is a joint loss combining cross-entropy and Dice loss, and\ \lambda=0.4 is the weight for auxiliary losses.

A. Main decoder (DVSS)
The decoder consists of four DVSS blocks: Block 4 refines F₄, while Blocks 3, 2, 1 progressively fuse upsampled features with skip features F_3^\prime,F_2^\prime,F_1^\prime via concatenation, 1\times1 projection, and DVSS processing. Each DVSS decouples global and local processing—the local path applies ECA for channel recalibration followed by PMC-modulated convolution\ (W_{eff}), then both paths are fused by APFG. Auxiliary heads at Blocks 2–4 provide deep supervision. 
To motivate our design, we revisit the baseline CSMamba decoder. As shown in Fig. 1(b), its core SS2D module captures global context via four-directional 2D-SSM scanning [4] and employs internal channel/spatial attention gates to enhance its feature selection capabilities. We posit that global semantic context modeling and local spatial detail refinement are distinct tasks, yet they share the same underlying feature foundation. Processing them sequentially within the single-path architecture of the original decoder may prevent either task from achieving optimal performance. The DVSS adopts a shared base, separate enhancement design, where global and local paths share the same feature source before task-specific optimization.
The structure of the DVSS is shown in Fig. 1(c). The input feature X\in\mathbb{R}^{\left(B\times H\times W\times C\right)} is first normalized and processed by the SS2D module to produce a shared foundational feature F_s. This feature serves both paths: the global path retains F_s directly as F_g, while the local detail path processes it as follows. It undergoes channel recalibration through an Efficient Channel Attention (ECA) [5] module, which models cross-channel dependencies via a lightweight 1D convolution. This recalibration helps the subsequent PMC focus on informative channels. The recalibrated feature is enhanced via a residual connection before being fed into the PMC module.
The PMC module is inspired by Linear Deformable Convolution (LDC) [6], but operates in the parameter domain rather than the spatial domain. This design choice is motivated by a key observation: standard convolutions exhibit a center-dominance tendency, where center weights consistently receive larger magnitudes after training. This occurs partly because the kernel center always corresponds to a well-aligned sampling position across spatial locations, and is repeatedly reinforced by overlapping receptive fields, leading to more stable gradient signals and larger cumulative updates during  
 
	 (a) The workflow of the PMC module. (b) The structure of the APFG module. (c) The structure of the MSK module.
 

 
optimization. Although a sufficiently expressive network could, in principle, learn non-center-dominant kernels from data, standard training does not explicitly encourage such behavior; consequently, gradient-based optimization tends to converge to center-reliant solutions that minimize training loss but remain suboptimal for fine-detail extraction. By shifting the deformation from the spatial to the parameter domain, PMC eliminates the complex coordinate offset and interpolation computations required in traditional LDC, while introducing a lightweight, learnable inhibitory prior to counteract center-dominance.
The core mechanism works as follows: a dynamic mask, correlated with the convolutional kernel weights, adaptively modulates the convolution weights and suppresses the intensity at the kernel's center. This suppression compels the network to gather information from neighboring positions, enhancing sensitivity to edges and fine textures—precisely the local details this path aims to capture.
As shown in Fig. 2(a), the weight tensor W of the standard convolution kernel is first summed across the spatial dimensions to generate a channel strength matrix S, which characterizes the importance of each input-output channel pair. Then, a dynamic mask M_d is generated by multiplying the learnable mask matrix W_m, the fixed center mask M_c (with a center value of 1 and zeros elsewhere), and the previously mentioned strength matrix S. A learnable scaling factor θ is then introduced, and the original convolution weights are modulated using the expression in Eq. (2), yielding the effective convolution weights W_{eff}.
Weff=W⊙B-θ×Md#2
where B is a tensor of all ones, and \odot represents element-wise multiplication. These modulated weights W_{eff} are applied to convolve the feature \left(ECA\left(F_s\right)+F_s\right), producing the local path output F_l.
Finally, the outputs of both paths are fused via the APFG module. As shown in Fig. 2(b), a shared APFG module applies channel-wise attention to each path independently: it performs global average pooling, followed by a bottleneck structure (reduction and expansion via linear layers) with sigmoid activation to generate attention weights. Each path's feature is recalibrated by its own attention weights, and the results are summed: 
Y=X+DropPathAPFGFg+APFGFl#3
We adopt SiLU in SS2D following the original VMamba [4] design for architectural consistency, while using GeLU in APFG for its smoother gradient profile suited to fusing features from heterogeneous paths.

B. Multi-Scale Fusion Module (MSK)
In the feature fusion stage, we reconsider the role of attention mechanisms and propose the more computationally efficient MSK module. This design is motivated by two key observations. First, the ECA module within the DVSS block's local detail path already accounts for inter-channel dependencies, making additional channel attention at fusion redundant. Second, the core challenge during this stage is bridging the semantic gap and spatial misalignment across multi-resolution feature maps, where integrating spatial information is far more critical than recalibrating channels. Thus, we discard complex hybrid attention in favor of a spatial-only feature fusion strategy.
Three MSK modules operate at different encoder scales to generate {F₁',F₂',F₃'}. For each MSK, the other two scales
